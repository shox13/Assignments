{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Neural Networks\n",
    "Neural Networks are another algorithm we can use to learn complicated,\n",
    "nonlinear relations among our data, and they work best with large amounts of \n",
    "data. \n",
    "\n",
    "Every neural net has an input layer, zero or more hidden layers, and one output\n",
    "layer. The input layer accepts the training examples, each hidden layer\n",
    "performs the calculation $\\sigma\\left(w^TX + b\\right)$, and the output layer\n",
    "performs the same calculation one last time. \n",
    "\n",
    "The simplest 1-layer neural net has only an input and output layer.\n",
    "Linear and logistic regression are also instances of 1-layer neural nets\n",
    "because they have an input layer that accepts training examples and an output\n",
    "layer that performs a calculation and outputs a value. \n",
    "\n",
    "## Infrastructure\n",
    "We'll load all libraries and pre-built functions here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def norm_data(X):\n",
    "\n",
    "    scaler = StandardScaler().fit(X)\n",
    "    X_norm = scaler.transform(X)\n",
    "\n",
    "    return X_norm\n",
    "\n",
    "def pca_transform(n, data, inv = False):\n",
    "    pca = PCA(n_components = n, random_state = 0)\n",
    "    data_norm = norm_data(data)\n",
    "\n",
    "    if inv:\n",
    "        Z = pca.inverse_transform(Z)*data.std(axis=0) + data.mean(axis=0)\n",
    "    else:\n",
    "        Z = pca.fit_transform(data_norm)\n",
    "\n",
    "    return Z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear and Logistic Regression as Neural Nets\n",
    "Let's take an algorithm we know-logistic regression-and describe it in terms of \n",
    "a neural net. \n",
    "\n",
    "![Logistic Regression as Neural Net source: deeplearning.ai](log_reg_as_nn.png){width=50%}\n",
    "\n",
    "We've seen previously that logistic regression takes the input examples (xi and\n",
    "yi), learns optimal parameters(wi and b), finds the output $w^TX + b$, \n",
    "squeezes the value into (0,1) via the sigmoid function, and finally\n",
    "classifies each example as 1 (pass) or 0 (fail) based on a typical 0.5\n",
    "probability threshold. \n",
    "\n",
    "Interpreting this as a neural net, the training examples (xi) are the input layer,\n",
    "and the \"neuron\" that computes $\\sigma\\left(w^TX + b\\right)$ is the output layer.\n",
    "In this way, we say logistic regression is also a 1-layer neural network. \n",
    "\n",
    "For linear regression, the interpretation of input and output layers is the\n",
    "same, except $w^TX + b$ is not passed through an activation function like the\n",
    "sigmoid function, $\\sigma$. \n",
    "\n",
    "An activation function changes the nature of the values $w^TX + b$. For\n",
    "example, if we want a probability, we send these values through the sigmoid \n",
    "activation function, which returns a value in (0,1). \n",
    "\n",
    "## Intro to Neural Network\n",
    "\n",
    "![Basic Neural Net source: Stanford CS231n](nn.png){width=50%}\n",
    "\n",
    "The only difference between linear and logistic regression and neural networks\n",
    "is the number of hidden layers. The smallest neural network has one hidden\n",
    "layer. The hidden layer computes $w^TX + b$ and passes it through an activation \n",
    "function. In the simplest case, these values are passed to the ouput layer, \n",
    "where another set of $w^TX + b$ values are computed and passed through another \n",
    "activation function such as the sigmoid function. These values are then\n",
    "classified as 1 (pass) or 0 (fail). \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Linear Classification with Keras\n",
    "Keras is a pre-built library which provides us a simple interface to implement\n",
    "neural networks. Let's try linear classification on the first two principal\n",
    "components of Boston Housing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "506/506 [==============================] - 2s 3ms/step - loss: 0.4270 - acc: 0.5553\n",
      "Epoch 2/100\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 0.2175 - acc: 0.7055\n",
      "Epoch 3/100\n",
      "506/506 [==============================] - 0s 850us/step - loss: 0.1577 - acc: 0.7925\n",
      "Epoch 4/100\n",
      "506/506 [==============================] - 0s 914us/step - loss: 0.1456 - acc: 0.8103\n",
      "Epoch 5/100\n",
      "506/506 [==============================] - 0s 913us/step - loss: 0.1442 - acc: 0.8083\n",
      "Epoch 6/100\n",
      "506/506 [==============================] - 0s 811us/step - loss: 0.1432 - acc: 0.8162\n",
      "Epoch 7/100\n",
      "506/506 [==============================] - 0s 930us/step - loss: 0.1435 - acc: 0.8103\n",
      "Epoch 8/100\n",
      "506/506 [==============================] - 0s 940us/step - loss: 0.1431 - acc: 0.8123\n",
      "Epoch 9/100\n",
      "506/506 [==============================] - 0s 927us/step - loss: 0.1432 - acc: 0.8123\n",
      "Epoch 10/100\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 0.1426 - acc: 0.8142\n",
      "Epoch 11/100\n",
      "506/506 [==============================] - 0s 825us/step - loss: 0.1432 - acc: 0.8142\n",
      "Epoch 12/100\n",
      "506/506 [==============================] - 0s 932us/step - loss: 0.1428 - acc: 0.8083\n",
      "Epoch 13/100\n",
      "506/506 [==============================] - 0s 984us/step - loss: 0.1432 - acc: 0.8142\n",
      "Epoch 14/100\n",
      "506/506 [==============================] - 0s 752us/step - loss: 0.1430 - acc: 0.8221\n",
      "Epoch 15/100\n",
      "506/506 [==============================] - 0s 906us/step - loss: 0.1430 - acc: 0.8142\n",
      "Epoch 16/100\n",
      "506/506 [==============================] - 0s 905us/step - loss: 0.1433 - acc: 0.8142\n",
      "Epoch 17/100\n",
      "506/506 [==============================] - 0s 811us/step - loss: 0.1428 - acc: 0.8103\n",
      "Epoch 18/100\n",
      "506/506 [==============================] - 0s 851us/step - loss: 0.1427 - acc: 0.8162\n",
      "Epoch 19/100\n",
      "506/506 [==============================] - 0s 860us/step - loss: 0.1427 - acc: 0.8202\n",
      "Epoch 20/100\n",
      "506/506 [==============================] - 0s 666us/step - loss: 0.1426 - acc: 0.8202\n",
      "Epoch 21/100\n",
      "506/506 [==============================] - 0s 695us/step - loss: 0.1426 - acc: 0.8162\n",
      "Epoch 22/100\n",
      "506/506 [==============================] - 0s 798us/step - loss: 0.1428 - acc: 0.8103\n",
      "Epoch 23/100\n",
      "506/506 [==============================] - 0s 826us/step - loss: 0.1429 - acc: 0.8103\n",
      "Epoch 24/100\n",
      "506/506 [==============================] - 0s 898us/step - loss: 0.1430 - acc: 0.8162\n",
      "Epoch 25/100\n",
      "506/506 [==============================] - 0s 981us/step - loss: 0.1430 - acc: 0.8083\n",
      "Epoch 26/100\n",
      "506/506 [==============================] - 0s 858us/step - loss: 0.1427 - acc: 0.8182\n",
      "Epoch 27/100\n",
      "506/506 [==============================] - 0s 927us/step - loss: 0.1424 - acc: 0.8142\n",
      "Epoch 28/100\n",
      "506/506 [==============================] - 0s 889us/step - loss: 0.1431 - acc: 0.8142\n",
      "Epoch 29/100\n",
      "506/506 [==============================] - 0s 735us/step - loss: 0.1425 - acc: 0.8103\n",
      "Epoch 30/100\n",
      "506/506 [==============================] - 0s 686us/step - loss: 0.1427 - acc: 0.8142\n",
      "Epoch 31/100\n",
      "506/506 [==============================] - 0s 721us/step - loss: 0.1430 - acc: 0.8103\n",
      "Epoch 32/100\n",
      "506/506 [==============================] - 0s 837us/step - loss: 0.1429 - acc: 0.8103\n",
      "Epoch 33/100\n",
      "506/506 [==============================] - 0s 878us/step - loss: 0.1429 - acc: 0.8103\n",
      "Epoch 34/100\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 0.1425 - acc: 0.8123A: 0s - loss: 0.1432 - acc: 0.81 - ETA: 0s - loss: 0.1507 - acc: \n",
      "Epoch 35/100\n",
      "506/506 [==============================] - 0s 717us/step - loss: 0.1430 - acc: 0.8202\n",
      "Epoch 36/100\n",
      "506/506 [==============================] - 0s 792us/step - loss: 0.1428 - acc: 0.8083\n",
      "Epoch 37/100\n",
      "506/506 [==============================] - 0s 704us/step - loss: 0.1433 - acc: 0.8142\n",
      "Epoch 38/100\n",
      "506/506 [==============================] - 0s 903us/step - loss: 0.1424 - acc: 0.8261\n",
      "Epoch 39/100\n",
      "506/506 [==============================] - 0s 834us/step - loss: 0.1432 - acc: 0.8083\n",
      "Epoch 40/100\n",
      "506/506 [==============================] - 0s 845us/step - loss: 0.1426 - acc: 0.8182\n",
      "Epoch 41/100\n",
      "506/506 [==============================] - 0s 755us/step - loss: 0.1426 - acc: 0.8162\n",
      "Epoch 42/100\n",
      "506/506 [==============================] - 0s 853us/step - loss: 0.1427 - acc: 0.8123\n",
      "Epoch 43/100\n",
      "506/506 [==============================] - 0s 877us/step - loss: 0.1427 - acc: 0.8142\n",
      "Epoch 44/100\n",
      "506/506 [==============================] - 0s 746us/step - loss: 0.1429 - acc: 0.8123\n",
      "Epoch 45/100\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 0.1436 - acc: 0.8142\n",
      "Epoch 46/100\n",
      "506/506 [==============================] - ETA: 0s - loss: 0.1370 - acc: 0.827 - 0s 721us/step - loss: 0.1427 - acc: 0.8182\n",
      "Epoch 47/100\n",
      "506/506 [==============================] - 0s 717us/step - loss: 0.1429 - acc: 0.8182\n",
      "Epoch 48/100\n",
      "506/506 [==============================] - 0s 717us/step - loss: 0.1428 - acc: 0.8182\n",
      "Epoch 49/100\n",
      "506/506 [==============================] - 0s 712us/step - loss: 0.1428 - acc: 0.8083\n",
      "Epoch 50/100\n",
      "506/506 [==============================] - 0s 667us/step - loss: 0.1429 - acc: 0.8123\n",
      "Epoch 51/100\n",
      "506/506 [==============================] - 0s 662us/step - loss: 0.1428 - acc: 0.8103\n",
      "Epoch 52/100\n",
      "506/506 [==============================] - 0s 813us/step - loss: 0.1427 - acc: 0.8162\n",
      "Epoch 53/100\n",
      "506/506 [==============================] - 0s 940us/step - loss: 0.1425 - acc: 0.8221\n",
      "Epoch 54/100\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 0.1430 - acc: 0.8142\n",
      "Epoch 55/100\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 0.1425 - acc: 0.8103\n",
      "Epoch 56/100\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 0.1430 - acc: 0.8063\n",
      "Epoch 57/100\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 0.1426 - acc: 0.8142\n",
      "Epoch 58/100\n",
      "506/506 [==============================] - 0s 612us/step - loss: 0.1427 - acc: 0.8142\n",
      "Epoch 59/100\n",
      "506/506 [==============================] - 0s 818us/step - loss: 0.1426 - acc: 0.8123\n",
      "Epoch 60/100\n",
      "506/506 [==============================] - 0s 853us/step - loss: 0.1429 - acc: 0.8103\n",
      "Epoch 61/100\n",
      "506/506 [==============================] - 0s 746us/step - loss: 0.1427 - acc: 0.8142\n",
      "Epoch 62/100\n",
      "506/506 [==============================] - 0s 850us/step - loss: 0.1427 - acc: 0.8103\n",
      "Epoch 63/100\n",
      "506/506 [==============================] - 0s 832us/step - loss: 0.1429 - acc: 0.8142\n",
      "Epoch 64/100\n",
      "506/506 [==============================] - 0s 843us/step - loss: 0.1432 - acc: 0.8123\n",
      "Epoch 65/100\n",
      "506/506 [==============================] - 0s 729us/step - loss: 0.1430 - acc: 0.8162\n",
      "Epoch 66/100\n",
      "506/506 [==============================] - 0s 845us/step - loss: 0.1425 - acc: 0.8142\n",
      "Epoch 67/100\n",
      "506/506 [==============================] - 0s 741us/step - loss: 0.1428 - acc: 0.8024\n",
      "Epoch 68/100\n",
      "506/506 [==============================] - 0s 686us/step - loss: 0.1429 - acc: 0.8142\n",
      "Epoch 69/100\n",
      "506/506 [==============================] - 0s 665us/step - loss: 0.1430 - acc: 0.8123\n",
      "Epoch 70/100\n",
      "506/506 [==============================] - 0s 699us/step - loss: 0.1428 - acc: 0.8162\n",
      "Epoch 71/100\n",
      "506/506 [==============================] - 0s 815us/step - loss: 0.1434 - acc: 0.8182\n",
      "Epoch 72/100\n",
      "506/506 [==============================] - 0s 927us/step - loss: 0.1428 - acc: 0.8182\n",
      "Epoch 73/100\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 0.1431 - acc: 0.8162\n",
      "Epoch 74/100\n",
      "506/506 [==============================] - 0s 724us/step - loss: 0.1430 - acc: 0.8083\n",
      "Epoch 75/100\n",
      "506/506 [==============================] - 0s 819us/step - loss: 0.1427 - acc: 0.8142\n",
      "Epoch 76/100\n",
      "506/506 [==============================] - 0s 839us/step - loss: 0.1432 - acc: 0.8123 0s - loss: 0.1416 - acc: 0.81\n",
      "Epoch 77/100\n",
      "506/506 [==============================] - 0s 686us/step - loss: 0.1426 - acc: 0.8221\n",
      "Epoch 78/100\n",
      "506/506 [==============================] - 0s 824us/step - loss: 0.1428 - acc: 0.8123\n",
      "Epoch 79/100\n",
      "506/506 [==============================] - 0s 824us/step - loss: 0.1429 - acc: 0.8123\n",
      "Epoch 80/100\n",
      "506/506 [==============================] - 0s 938us/step - loss: 0.1427 - acc: 0.8162\n",
      "Epoch 81/100\n",
      "506/506 [==============================] - 0s 715us/step - loss: 0.1425 - acc: 0.8123\n",
      "Epoch 82/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "506/506 [==============================] - ETA: 0s - loss: 0.1406 - acc: 0.819 - 0s 717us/step - loss: 0.1425 - acc: 0.8162\n",
      "Epoch 83/100\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 0.1425 - acc: 0.8162\n",
      "Epoch 84/100\n",
      "506/506 [==============================] - 0s 833us/step - loss: 0.1426 - acc: 0.8083\n",
      "Epoch 85/100\n",
      "506/506 [==============================] - 0s 933us/step - loss: 0.1429 - acc: 0.8162\n",
      "Epoch 86/100\n",
      "506/506 [==============================] - 0s 808us/step - loss: 0.1427 - acc: 0.8162\n",
      "Epoch 87/100\n",
      "506/506 [==============================] - 0s 814us/step - loss: 0.1424 - acc: 0.8162\n",
      "Epoch 88/100\n",
      "506/506 [==============================] - 0s 904us/step - loss: 0.1430 - acc: 0.8123\n",
      "Epoch 89/100\n",
      "506/506 [==============================] - 0s 801us/step - loss: 0.1430 - acc: 0.8162\n",
      "Epoch 90/100\n",
      "506/506 [==============================] - 0s 760us/step - loss: 0.1426 - acc: 0.8123\n",
      "Epoch 91/100\n",
      "506/506 [==============================] - 0s 739us/step - loss: 0.1428 - acc: 0.8083\n",
      "Epoch 92/100\n",
      "506/506 [==============================] - 0s 929us/step - loss: 0.1429 - acc: 0.8162\n",
      "Epoch 93/100\n",
      "506/506 [==============================] - 0s 876us/step - loss: 0.1428 - acc: 0.8162\n",
      "Epoch 94/100\n",
      "506/506 [==============================] - 0s 949us/step - loss: 0.1419 - acc: 0.8103\n",
      "Epoch 95/100\n",
      "506/506 [==============================] - 0s 640us/step - loss: 0.1429 - acc: 0.8103\n",
      "Epoch 96/100\n",
      "506/506 [==============================] - 0s 687us/step - loss: 0.1425 - acc: 0.8103\n",
      "Epoch 97/100\n",
      "506/506 [==============================] - 0s 752us/step - loss: 0.1426 - acc: 0.8083\n",
      "Epoch 98/100\n",
      "506/506 [==============================] - 0s 799us/step - loss: 0.1430 - acc: 0.8162\n",
      "Epoch 99/100\n",
      "506/506 [==============================] - 0s 935us/step - loss: 0.1431 - acc: 0.8103 0s - loss: 0.1368 - acc:\n",
      "Epoch 100/100\n",
      "506/506 [==============================] - 0s 648us/step - loss: 0.1430 - acc: 0.8142\n",
      "506/506 [==============================] - 0s 282us/step\n",
      "\n",
      "Loss: 0.14, Accuracy: 81.42%\n"
     ]
    }
   ],
   "source": [
    "boston = load_boston()\n",
    "X, y = boston.data, boston.target\n",
    "y_mean = np.where(y>y.mean(), 1, 0)\n",
    "X_2D = pca_transform(2, X)\n",
    "\n",
    "# lin. classification in terms of neural net\n",
    "model = Sequential()\n",
    "# this adds the input and output layers\n",
    "# relu is a different choice than sigmoid for activation function\n",
    "model.add(Dense(1, input_dim=2, activation='relu'))\n",
    "# mse (mean squared error) is the loss function used for linear regression\n",
    "# sgd (stochastic gradient descent) is the optimization algorithm used to \n",
    "# find the optimal parameters W\n",
    "model.compile(loss='mse', optimizer='sgd', metrics=['accuracy'])\n",
    "# epochs are the number of iterations of sgd to run\n",
    "# batch size is used to control how many examples are trained at a time\n",
    "model.fit(X_2D, y_mean, epochs=100, batch_size=10)\n",
    "loss, _ = model.evaluate(X_2D, y_mean)\n",
    "# predict returns a probability in (0,1)\n",
    "probabilities = model.predict(X_2D)\n",
    "# this converts probs. to 0 or 1\n",
    "predictions = [float(np.round(x)) for x in probabilities]\n",
    "accuracy = np.mean(predictions == y_mean)\n",
    "print(\"\\nLoss: %.2f, Accuracy: %.2f%%\" % (loss, accuracy*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Classification with Keras\n",
    "Now, let's implement logistic regression in a similar manner:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "506/506 [==============================] - 2s 3ms/step - loss: 1.0005 - acc: 0.3182\n",
      "Epoch 2/100\n",
      "506/506 [==============================] - 0s 949us/step - loss: 0.9449 - acc: 0.3202\n",
      "Epoch 3/100\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 0.8932 - acc: 0.3261\n",
      "Epoch 4/100\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 0.8460 - acc: 0.3360\n",
      "Epoch 5/100\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 0.8030 - acc: 0.3597\n",
      "Epoch 6/100\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 0.7640 - acc: 0.3735\n",
      "Epoch 7/100\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 0.7300 - acc: 0.4032\n",
      "Epoch 8/100\n",
      "506/506 [==============================] - 0s 908us/step - loss: 0.6997 - acc: 0.5316\n",
      "Epoch 9/100\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 0.6733 - acc: 0.6462\n",
      "Epoch 10/100\n",
      "506/506 [==============================] - 0s 820us/step - loss: 0.6501 - acc: 0.6798\n",
      "Epoch 11/100\n",
      "506/506 [==============================] - 0s 862us/step - loss: 0.6299 - acc: 0.6996\n",
      "Epoch 12/100\n",
      "506/506 [==============================] - 0s 844us/step - loss: 0.6124 - acc: 0.7233\n",
      "Epoch 13/100\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 0.5968 - acc: 0.7510\n",
      "Epoch 14/100\n",
      "506/506 [==============================] - 0s 931us/step - loss: 0.5836 - acc: 0.7589\n",
      "Epoch 15/100\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 0.5720 - acc: 0.7727\n",
      "Epoch 16/100\n",
      "506/506 [==============================] - 0s 908us/step - loss: 0.5617 - acc: 0.7767\n",
      "Epoch 17/100\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 0.5528 - acc: 0.7826\n",
      "Epoch 18/100\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 0.5450 - acc: 0.7826A: 0s - loss: 0.5229 - ac\n",
      "Epoch 19/100\n",
      "506/506 [==============================] - ETA: 0s - loss: 0.5394 - acc: 0.781 - 1s 1ms/step - loss: 0.5379 - acc: 0.7826\n",
      "Epoch 20/100\n",
      "506/506 [==============================] - 0s 903us/step - loss: 0.5319 - acc: 0.7826\n",
      "Epoch 21/100\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 0.5265 - acc: 0.7846A: 0s - loss: 0.5424 - ac\n",
      "Epoch 22/100\n",
      "506/506 [==============================] - 0s 834us/step - loss: 0.5217 - acc: 0.7905 0s - loss: 0.5414 - acc:\n",
      "Epoch 23/100\n",
      "506/506 [==============================] - 0s 782us/step - loss: 0.5174 - acc: 0.7846\n",
      "Epoch 24/100\n",
      "506/506 [==============================] - 0s 846us/step - loss: 0.5136 - acc: 0.7826\n",
      "Epoch 25/100\n",
      "506/506 [==============================] - 0s 965us/step - loss: 0.5101 - acc: 0.7826\n",
      "Epoch 26/100\n",
      "506/506 [==============================] - 0s 858us/step - loss: 0.5071 - acc: 0.7826\n",
      "Epoch 27/100\n",
      "506/506 [==============================] - 0s 757us/step - loss: 0.5043 - acc: 0.7846\n",
      "Epoch 28/100\n",
      "506/506 [==============================] - 0s 757us/step - loss: 0.5018 - acc: 0.7826\n",
      "Epoch 29/100\n",
      "506/506 [==============================] - 0s 963us/step - loss: 0.4994 - acc: 0.7866\n",
      "Epoch 30/100\n",
      "506/506 [==============================] - 0s 862us/step - loss: 0.4973 - acc: 0.7885\n",
      "Epoch 31/100\n",
      "506/506 [==============================] - 0s 865us/step - loss: 0.4955 - acc: 0.7885\n",
      "Epoch 32/100\n",
      "506/506 [==============================] - 0s 727us/step - loss: 0.4937 - acc: 0.7885\n",
      "Epoch 33/100\n",
      "506/506 [==============================] - 0s 768us/step - loss: 0.4923 - acc: 0.7885\n",
      "Epoch 34/100\n",
      "506/506 [==============================] - 0s 839us/step - loss: 0.4908 - acc: 0.7905\n",
      "Epoch 35/100\n",
      "506/506 [==============================] - 0s 815us/step - loss: 0.4895 - acc: 0.7885\n",
      "Epoch 36/100\n",
      "506/506 [==============================] - 0s 845us/step - loss: 0.4882 - acc: 0.7885\n",
      "Epoch 37/100\n",
      "506/506 [==============================] - 0s 823us/step - loss: 0.4870 - acc: 0.7866\n",
      "Epoch 38/100\n",
      "506/506 [==============================] - 0s 931us/step - loss: 0.4860 - acc: 0.7866\n",
      "Epoch 39/100\n",
      "506/506 [==============================] - 0s 969us/step - loss: 0.4850 - acc: 0.7885\n",
      "Epoch 40/100\n",
      "506/506 [==============================] - 0s 934us/step - loss: 0.4841 - acc: 0.7885\n",
      "Epoch 41/100\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 0.4833 - acc: 0.7905\n",
      "Epoch 42/100\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 0.4825 - acc: 0.7885\n",
      "Epoch 43/100\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 0.4818 - acc: 0.7866\n",
      "Epoch 44/100\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 0.4811 - acc: 0.7885\n",
      "Epoch 45/100\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 0.4805 - acc: 0.7925\n",
      "Epoch 46/100\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 0.4800 - acc: 0.7925\n",
      "Epoch 47/100\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 0.4793 - acc: 0.7925\n",
      "Epoch 48/100\n",
      "506/506 [==============================] - 0s 837us/step - loss: 0.4788 - acc: 0.7925\n",
      "Epoch 49/100\n",
      "506/506 [==============================] - 0s 842us/step - loss: 0.4784 - acc: 0.7925\n",
      "Epoch 50/100\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 0.4780 - acc: 0.7964\n",
      "Epoch 51/100\n",
      "506/506 [==============================] - 0s 921us/step - loss: 0.4775 - acc: 0.7964\n",
      "Epoch 52/100\n",
      "506/506 [==============================] - 0s 937us/step - loss: 0.4771 - acc: 0.7984\n",
      "Epoch 53/100\n",
      "506/506 [==============================] - 0s 907us/step - loss: 0.4768 - acc: 0.7945\n",
      "Epoch 54/100\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 0.4764 - acc: 0.7964\n",
      "Epoch 55/100\n",
      "506/506 [==============================] - ETA: 0s - loss: 0.4649 - acc: 0.806 - 0s 836us/step - loss: 0.4761 - acc: 0.8004\n",
      "Epoch 56/100\n",
      "506/506 [==============================] - 0s 791us/step - loss: 0.4758 - acc: 0.7984\n",
      "Epoch 57/100\n",
      "506/506 [==============================] - 0s 973us/step - loss: 0.4756 - acc: 0.7984\n",
      "Epoch 58/100\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 0.4752 - acc: 0.8004\n",
      "Epoch 59/100\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 0.4749 - acc: 0.8004A: 0s - loss: 0.4712 - ac\n",
      "Epoch 60/100\n",
      "506/506 [==============================] - 0s 925us/step - loss: 0.4746 - acc: 0.7984\n",
      "Epoch 61/100\n",
      "506/506 [==============================] - 0s 916us/step - loss: 0.4744 - acc: 0.7984\n",
      "Epoch 62/100\n",
      "506/506 [==============================] - 0s 942us/step - loss: 0.4742 - acc: 0.7984\n",
      "Epoch 63/100\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 0.4740 - acc: 0.7984\n",
      "Epoch 64/100\n",
      "506/506 [==============================] - 0s 786us/step - loss: 0.4739 - acc: 0.7964\n",
      "Epoch 65/100\n",
      "506/506 [==============================] - 0s 925us/step - loss: 0.4736 - acc: 0.7984\n",
      "Epoch 66/100\n",
      "506/506 [==============================] - 1s 996us/step - loss: 0.4735 - acc: 0.7984\n",
      "Epoch 67/100\n",
      "506/506 [==============================] - 0s 929us/step - loss: 0.4733 - acc: 0.7984\n",
      "Epoch 68/100\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 0.4731 - acc: 0.7964\n",
      "Epoch 69/100\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 0.4730 - acc: 0.7964\n",
      "Epoch 70/100\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 0.4729 - acc: 0.7964A: 0s - loss: 0.4785 - acc:\n",
      "Epoch 71/100\n",
      "506/506 [==============================] - 0s 852us/step - loss: 0.4728 - acc: 0.7984\n",
      "Epoch 72/100\n",
      "506/506 [==============================] - 0s 902us/step - loss: 0.4726 - acc: 0.7984\n",
      "Epoch 73/100\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 0.4726 - acc: 0.8004\n",
      "Epoch 74/100\n",
      "506/506 [==============================] - 0s 893us/step - loss: 0.4725 - acc: 0.8004\n",
      "Epoch 75/100\n",
      "506/506 [==============================] - 0s 849us/step - loss: 0.4723 - acc: 0.7984\n",
      "Epoch 76/100\n",
      "506/506 [==============================] - 0s 830us/step - loss: 0.4723 - acc: 0.8004\n",
      "Epoch 77/100\n",
      "506/506 [==============================] - 0s 724us/step - loss: 0.4721 - acc: 0.8004\n",
      "Epoch 78/100\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 0.4721 - acc: 0.8004\n",
      "Epoch 79/100\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 0.4720 - acc: 0.8004\n",
      "Epoch 80/100\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 0.4719 - acc: 0.8004\n",
      "Epoch 81/100\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 0.4718 - acc: 0.8004\n",
      "Epoch 82/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "506/506 [==============================] - 0s 870us/step - loss: 0.4717 - acc: 0.8004\n",
      "Epoch 83/100\n",
      "506/506 [==============================] - 0s 940us/step - loss: 0.4716 - acc: 0.8004\n",
      "Epoch 84/100\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 0.4716 - acc: 0.8004\n",
      "Epoch 85/100\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 0.4716 - acc: 0.8004\n",
      "Epoch 86/100\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 0.4715 - acc: 0.8004\n",
      "Epoch 87/100\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 0.4715 - acc: 0.8004\n",
      "Epoch 88/100\n",
      "506/506 [==============================] - 0s 907us/step - loss: 0.4714 - acc: 0.8004\n",
      "Epoch 89/100\n",
      "506/506 [==============================] - 1s 996us/step - loss: 0.4714 - acc: 0.8004\n",
      "Epoch 90/100\n",
      "506/506 [==============================] - 0s 799us/step - loss: 0.4714 - acc: 0.8004\n",
      "Epoch 91/100\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 0.4714 - acc: 0.8004\n",
      "Epoch 92/100\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 0.4712 - acc: 0.8004\n",
      "Epoch 93/100\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 0.4711 - acc: 0.8024\n",
      "Epoch 94/100\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 0.4711 - acc: 0.8024\n",
      "Epoch 95/100\n",
      "506/506 [==============================] - 0s 942us/step - loss: 0.4712 - acc: 0.8024\n",
      "Epoch 96/100\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 0.4711 - acc: 0.8024\n",
      "Epoch 97/100\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 0.4712 - acc: 0.8004\n",
      "Epoch 98/100\n",
      "506/506 [==============================] - 0s 959us/step - loss: 0.4711 - acc: 0.8024\n",
      "Epoch 99/100\n",
      "506/506 [==============================] - 0s 793us/step - loss: 0.4710 - acc: 0.8024\n",
      "Epoch 100/100\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 0.4710 - acc: 0.8024\n",
      "506/506 [==============================] - 0s 637us/step\n",
      "\n",
      "Loss: 0.47, Accuracy: 80.24%\n"
     ]
    }
   ],
   "source": [
    "# log. classification in terms of neural net\n",
    "model = Sequential()\n",
    "model.add(Dense(1, input_dim=2, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X_2D, y_mean, epochs=100, batch_size=10)\n",
    "loss, _ = model.evaluate(X_2D, y_mean)\n",
    "probabilities = model.predict(X_2D)\n",
    "predictions = [float(np.round(x)) for x in probabilities]\n",
    "accuracy = np.mean(predictions == y_mean)\n",
    "print(\"\\nLoss: %.2f, Accuracy: %.2f%%\" % (loss, accuracy*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Net with Keras\n",
    "Time to add some hidden layers and create a neural net!:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "506/506 [==============================] - 2s 5ms/step - loss: 0.6445 - acc: 0.6917\n",
      "Epoch 2/100\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 0.5876 - acc: 0.7134\n",
      "Epoch 3/100\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 0.5469 - acc: 0.7292\n",
      "Epoch 4/100\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 0.5200 - acc: 0.7372\n",
      "Epoch 5/100\n",
      "506/506 [==============================] - 1s 2ms/step - loss: 0.5046 - acc: 0.7628\n",
      "Epoch 6/100\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 0.4943 - acc: 0.7747\n",
      "Epoch 7/100\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 0.4880 - acc: 0.7885\n",
      "Epoch 8/100\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 0.4834 - acc: 0.7984\n",
      "Epoch 9/100\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 0.4796 - acc: 0.7984\n",
      "Epoch 10/100\n",
      "506/506 [==============================] - 1s 2ms/step - loss: 0.4763 - acc: 0.7964\n",
      "Epoch 11/100\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 0.4730 - acc: 0.7945\n",
      "Epoch 12/100\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 0.4706 - acc: 0.7964\n",
      "Epoch 13/100\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 0.4676 - acc: 0.8043\n",
      "Epoch 14/100\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 0.4653 - acc: 0.8004A: 0s - loss: 0.3678 - a\n",
      "Epoch 15/100\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 0.4623 - acc: 0.8063\n",
      "Epoch 16/100\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 0.4600 - acc: 0.8103\n",
      "Epoch 17/100\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 0.4576 - acc: 0.8123\n",
      "Epoch 18/100\n",
      "506/506 [==============================] - 0s 921us/step - loss: 0.4561 - acc: 0.8083\n",
      "Epoch 19/100\n",
      "506/506 [==============================] - 0s 900us/step - loss: 0.4535 - acc: 0.8162\n",
      "Epoch 20/100\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 0.4514 - acc: 0.8162\n",
      "Epoch 21/100\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 0.4500 - acc: 0.8162\n",
      "Epoch 22/100\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 0.4481 - acc: 0.8202\n",
      "Epoch 23/100\n",
      "506/506 [==============================] - 0s 984us/step - loss: 0.4461 - acc: 0.8182\n",
      "Epoch 24/100\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 0.4447 - acc: 0.8182\n",
      "Epoch 25/100\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 0.4432 - acc: 0.8241\n",
      "Epoch 26/100\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 0.4413 - acc: 0.8241\n",
      "Epoch 27/100\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 0.4399 - acc: 0.8241\n",
      "Epoch 28/100\n",
      "506/506 [==============================] - 0s 869us/step - loss: 0.4389 - acc: 0.8221\n",
      "Epoch 29/100\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 0.4370 - acc: 0.8261\n",
      "Epoch 30/100\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 0.4357 - acc: 0.8281\n",
      "Epoch 31/100\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 0.4343 - acc: 0.8261A: 0s - loss: 0.3959 - ac\n",
      "Epoch 32/100\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 0.4330 - acc: 0.8300\n",
      "Epoch 33/100\n",
      "506/506 [==============================] - 0s 967us/step - loss: 0.4316 - acc: 0.8320\n",
      "Epoch 34/100\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 0.4312 - acc: 0.8241\n",
      "Epoch 35/100\n",
      "506/506 [==============================] - 0s 835us/step - loss: 0.4295 - acc: 0.8182\n",
      "Epoch 36/100\n",
      "506/506 [==============================] - ETA: 0s - loss: 0.4204 - acc: 0.827 - 0s 965us/step - loss: 0.4287 - acc: 0.8221\n",
      "Epoch 37/100\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 0.4273 - acc: 0.8202\n",
      "Epoch 38/100\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 0.4266 - acc: 0.8182\n",
      "Epoch 39/100\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 0.4260 - acc: 0.8241\n",
      "Epoch 40/100\n",
      "506/506 [==============================] - 0s 978us/step - loss: 0.4247 - acc: 0.8182\n",
      "Epoch 41/100\n",
      "506/506 [==============================] - 1s 990us/step - loss: 0.4238 - acc: 0.8202\n",
      "Epoch 42/100\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 0.4240 - acc: 0.8182\n",
      "Epoch 43/100\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 0.4226 - acc: 0.8142\n",
      "Epoch 44/100\n",
      "506/506 [==============================] - 0s 955us/step - loss: 0.4229 - acc: 0.8202\n",
      "Epoch 45/100\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 0.4216 - acc: 0.8202\n",
      "Epoch 46/100\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 0.4212 - acc: 0.8202\n",
      "Epoch 47/100\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 0.4207 - acc: 0.8221\n",
      "Epoch 48/100\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 0.4200 - acc: 0.8142\n",
      "Epoch 49/100\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 0.4199 - acc: 0.8162\n",
      "Epoch 50/100\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 0.4203 - acc: 0.8162\n",
      "Epoch 51/100\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 0.4192 - acc: 0.8182\n",
      "Epoch 52/100\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 0.4187 - acc: 0.8182\n",
      "Epoch 53/100\n",
      "506/506 [==============================] - ETA: 0s - loss: 0.4214 - acc: 0.822 - 1s 1ms/step - loss: 0.4184 - acc: 0.8241\n",
      "Epoch 54/100\n",
      "506/506 [==============================] - 0s 984us/step - loss: 0.4177 - acc: 0.8221\n",
      "Epoch 55/100\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 0.4178 - acc: 0.8221\n",
      "Epoch 56/100\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 0.4178 - acc: 0.8202\n",
      "Epoch 57/100\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 0.4171 - acc: 0.8202\n",
      "Epoch 58/100\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 0.4174 - acc: 0.8182\n",
      "Epoch 59/100\n",
      "506/506 [==============================] - 1s 996us/step - loss: 0.4170 - acc: 0.8182\n",
      "Epoch 60/100\n",
      "506/506 [==============================] - 0s 980us/step - loss: 0.4168 - acc: 0.8221\n",
      "Epoch 61/100\n",
      "506/506 [==============================] - 0s 855us/step - loss: 0.4165 - acc: 0.8162\n",
      "Epoch 62/100\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 0.4157 - acc: 0.8182\n",
      "Epoch 63/100\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 0.4163 - acc: 0.8241\n",
      "Epoch 64/100\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 0.4154 - acc: 0.8221\n",
      "Epoch 65/100\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 0.4154 - acc: 0.8221\n",
      "Epoch 66/100\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 0.4154 - acc: 0.8261\n",
      "Epoch 67/100\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 0.4151 - acc: 0.8221\n",
      "Epoch 68/100\n",
      "506/506 [==============================] - 0s 908us/step - loss: 0.4152 - acc: 0.8202\n",
      "Epoch 69/100\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 0.4146 - acc: 0.8241\n",
      "Epoch 70/100\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 0.4145 - acc: 0.8241\n",
      "Epoch 71/100\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 0.4140 - acc: 0.8241\n",
      "Epoch 72/100\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 0.4140 - acc: 0.8241\n",
      "Epoch 73/100\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 0.4142 - acc: 0.8241\n",
      "Epoch 74/100\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 0.4137 - acc: 0.8241\n",
      "Epoch 75/100\n",
      "506/506 [==============================] - 0s 835us/step - loss: 0.4138 - acc: 0.8221\n",
      "Epoch 76/100\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 0.4136 - acc: 0.8261\n",
      "Epoch 77/100\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 0.4130 - acc: 0.8241\n",
      "Epoch 78/100\n",
      "506/506 [==============================] - 1s 999us/step - loss: 0.4133 - acc: 0.8221\n",
      "Epoch 79/100\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 0.4127 - acc: 0.8221\n",
      "Epoch 80/100\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 0.4129 - acc: 0.8221\n",
      "Epoch 81/100\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 0.4129 - acc: 0.8221\n",
      "Epoch 82/100\n",
      "506/506 [==============================] - ETA: 0s - loss: 0.4150 - acc: 0.820 - 1s 1ms/step - loss: 0.4127 - acc: 0.8221\n",
      "Epoch 83/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "506/506 [==============================] - 1s 1ms/step - loss: 0.4129 - acc: 0.8221\n",
      "Epoch 84/100\n",
      "506/506 [==============================] - 0s 956us/step - loss: 0.4124 - acc: 0.8241\n",
      "Epoch 85/100\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 0.4121 - acc: 0.8221\n",
      "Epoch 86/100\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 0.4131 - acc: 0.8241\n",
      "Epoch 87/100\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 0.4131 - acc: 0.8221\n",
      "Epoch 88/100\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 0.4132 - acc: 0.8162\n",
      "Epoch 89/100\n",
      "506/506 [==============================] - 1s 992us/step - loss: 0.4123 - acc: 0.8241\n",
      "Epoch 90/100\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 0.4120 - acc: 0.8202\n",
      "Epoch 91/100\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 0.4124 - acc: 0.8202\n",
      "Epoch 92/100\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 0.4117 - acc: 0.8241\n",
      "Epoch 93/100\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 0.4124 - acc: 0.8182\n",
      "Epoch 94/100\n",
      "506/506 [==============================] - 0s 907us/step - loss: 0.4118 - acc: 0.8221\n",
      "Epoch 95/100\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 0.4112 - acc: 0.8202\n",
      "Epoch 96/100\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 0.4116 - acc: 0.8221\n",
      "Epoch 97/100\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 0.4115 - acc: 0.8241\n",
      "Epoch 98/100\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 0.4116 - acc: 0.8182\n",
      "Epoch 99/100\n",
      "506/506 [==============================] - 0s 854us/step - loss: 0.4112 - acc: 0.8182\n",
      "Epoch 100/100\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 0.4113 - acc: 0.8202\n",
      "506/506 [==============================] - 0s 675us/step\n",
      "\n",
      "Loss: 0.41, Accuracy: 82.02%\n"
     ]
    }
   ],
   "source": [
    "# neural net on same data\n",
    "model = Sequential()\n",
    "# this adds the input layer and a 5 neuron hidden layer\n",
    "model.add(Dense(5,input_dim=2, activation='relu'))\n",
    "# this adds the one-neuron output layer\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "# binary crossentropy is the loss used in logistic regression \n",
    "# adam is a substitute for gradient descent to find the optimal parameters, W.\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X_2D, y_mean, epochs=100, batch_size=10)\n",
    "loss, _ = model.evaluate(X_2D, y_mean)\n",
    "probabilities = model.predict(X_2D)\n",
    "predictions = [float(np.round(x)) for x in probabilities]\n",
    "accuracy = np.mean(predictions == y_mean)\n",
    "print(\"\\nLoss: %.2f, Accuracy: %.2f%%\" % (loss, accuracy*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "We've seen how to build a neural net with Keras by adding layers. Now, your\n",
    "task is to build a 3-layer neural net on Boston Housing by additing one \n",
    "additional layer. Use the previous examples as a guide and experiment with the\n",
    "number of neurons in each layer to try to get a better fit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "506/506 [==============================] - 3s 5ms/step - loss: 0.6548 - acc: 0.6542\n",
      "Epoch 2/100\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 0.6038 - acc: 0.6759\n",
      "Epoch 3/100\n",
      "506/506 [==============================] - 1s 2ms/step - loss: 0.5795 - acc: 0.6719\n",
      "Epoch 4/100\n",
      "506/506 [==============================] - 1s 2ms/step - loss: 0.5662 - acc: 0.6700\n",
      "Epoch 5/100\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 0.5595 - acc: 0.6739\n",
      "Epoch 6/100\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 0.5549 - acc: 0.6719A: 0s - loss: 0.6165 - ac\n",
      "Epoch 7/100\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 0.5518 - acc: 0.6779\n",
      "Epoch 8/100\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 0.5496 - acc: 0.6759\n",
      "Epoch 9/100\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 0.5478 - acc: 0.6759\n",
      "Epoch 10/100\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 0.5464 - acc: 0.6759\n",
      "Epoch 11/100\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 0.5448 - acc: 0.6779\n",
      "Epoch 12/100\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 0.5434 - acc: 0.6739\n",
      "Epoch 13/100\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 0.5422 - acc: 0.6818\n",
      "Epoch 14/100\n",
      "506/506 [==============================] - 0s 949us/step - loss: 0.5414 - acc: 0.6818\n",
      "Epoch 15/100\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 0.5402 - acc: 0.6818\n",
      "Epoch 16/100\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 0.5390 - acc: 0.6858\n",
      "Epoch 17/100\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 0.5379 - acc: 0.6838\n",
      "Epoch 18/100\n",
      "506/506 [==============================] - 1s 2ms/step - loss: 0.5368 - acc: 0.6858\n",
      "Epoch 19/100\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 0.5359 - acc: 0.6897\n",
      "Epoch 20/100\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 0.5351 - acc: 0.6937\n",
      "Epoch 21/100\n",
      "506/506 [==============================] - 1s 2ms/step - loss: 0.5337 - acc: 0.6976\n",
      "Epoch 22/100\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 0.5323 - acc: 0.6976\n",
      "Epoch 23/100\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 0.5309 - acc: 0.7549\n",
      "Epoch 24/100\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 0.5298 - acc: 0.7549A: 0s - loss: 0.5754 - \n",
      "Epoch 25/100\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 0.5283 - acc: 0.7549\n",
      "Epoch 26/100\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 0.5269 - acc: 0.7569\n",
      "Epoch 27/100\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 0.5250 - acc: 0.7549\n",
      "Epoch 28/100\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 0.5232 - acc: 0.7549\n",
      "Epoch 29/100\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 0.5212 - acc: 0.7569\n",
      "Epoch 30/100\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 0.5189 - acc: 0.7589\n",
      "Epoch 31/100\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 0.5159 - acc: 0.7609\n",
      "Epoch 32/100\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 0.5116 - acc: 0.7628\n",
      "Epoch 33/100\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 0.5055 - acc: 0.7668\n",
      "Epoch 34/100\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 0.4994 - acc: 0.7688\n",
      "Epoch 35/100\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 0.4925 - acc: 0.7747\n",
      "Epoch 36/100\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 0.4870 - acc: 0.7747\n",
      "Epoch 37/100\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 0.4820 - acc: 0.7767\n",
      "Epoch 38/100\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 0.4769 - acc: 0.7767\n",
      "Epoch 39/100\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 0.4718 - acc: 0.7747\n",
      "Epoch 40/100\n",
      "506/506 [==============================] - 1s 2ms/step - loss: 0.4678 - acc: 0.7767\n",
      "Epoch 41/100\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 0.4627 - acc: 0.7826\n",
      "Epoch 42/100\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 0.4562 - acc: 0.7905\n",
      "Epoch 43/100\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 0.4528 - acc: 0.7964\n",
      "Epoch 44/100\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 0.4499 - acc: 0.7945\n",
      "Epoch 45/100\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 0.4471 - acc: 0.7964\n",
      "Epoch 46/100\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 0.4446 - acc: 0.7984\n",
      "Epoch 47/100\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 0.4422 - acc: 0.8004\n",
      "Epoch 48/100\n",
      "506/506 [==============================] - 0s 974us/step - loss: 0.4402 - acc: 0.8024\n",
      "Epoch 49/100\n",
      "506/506 [==============================] - 1s 2ms/step - loss: 0.4376 - acc: 0.8024\n",
      "Epoch 50/100\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 0.4349 - acc: 0.8202\n",
      "Epoch 51/100\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 0.4324 - acc: 0.8142\n",
      "Epoch 52/100\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 0.4301 - acc: 0.8123\n",
      "Epoch 53/100\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 0.4289 - acc: 0.8123\n",
      "Epoch 54/100\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 0.4276 - acc: 0.8221\n",
      "Epoch 55/100\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 0.4265 - acc: 0.8261\n",
      "Epoch 56/100\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 0.4249 - acc: 0.8241\n",
      "Epoch 57/100\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 0.4244 - acc: 0.8221\n",
      "Epoch 58/100\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 0.4243 - acc: 0.8221\n",
      "Epoch 59/100\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 0.4238 - acc: 0.8261\n",
      "Epoch 60/100\n",
      "506/506 [==============================] - 0s 909us/step - loss: 0.4228 - acc: 0.8281\n",
      "Epoch 61/100\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 0.4222 - acc: 0.8221A: 0s - loss: 0.2985 -\n",
      "Epoch 62/100\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 0.4231 - acc: 0.8202\n",
      "Epoch 63/100\n",
      "506/506 [==============================] - 0s 929us/step - loss: 0.4209 - acc: 0.8261\n",
      "Epoch 64/100\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 0.4213 - acc: 0.8241\n",
      "Epoch 65/100\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 0.4211 - acc: 0.8281\n",
      "Epoch 66/100\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 0.4214 - acc: 0.8221\n",
      "Epoch 67/100\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 0.4210 - acc: 0.8221\n",
      "Epoch 68/100\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 0.4195 - acc: 0.8261\n",
      "Epoch 69/100\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 0.4197 - acc: 0.8281\n",
      "Epoch 70/100\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 0.4194 - acc: 0.8261\n",
      "Epoch 71/100\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 0.4184 - acc: 0.8261\n",
      "Epoch 72/100\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 0.4198 - acc: 0.8221\n",
      "Epoch 73/100\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 0.4185 - acc: 0.8202\n",
      "Epoch 74/100\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 0.4182 - acc: 0.8241\n",
      "Epoch 75/100\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 0.4182 - acc: 0.8241\n",
      "Epoch 76/100\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 0.4185 - acc: 0.8241\n",
      "Epoch 77/100\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 0.4179 - acc: 0.8202\n",
      "Epoch 78/100\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 0.4185 - acc: 0.8202\n",
      "Epoch 79/100\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 0.4179 - acc: 0.8241\n",
      "Epoch 80/100\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 0.4173 - acc: 0.8261\n",
      "Epoch 81/100\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 0.4166 - acc: 0.8241\n",
      "Epoch 82/100\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 0.4170 - acc: 0.8221\n",
      "Epoch 83/100\n",
      "506/506 [==============================] - 0s 871us/step - loss: 0.4165 - acc: 0.8281\n",
      "Epoch 84/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "506/506 [==============================] - 0s 946us/step - loss: 0.4166 - acc: 0.8241\n",
      "Epoch 85/100\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 0.4165 - acc: 0.8261\n",
      "Epoch 86/100\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 0.4167 - acc: 0.8261\n",
      "Epoch 87/100\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 0.4161 - acc: 0.8261\n",
      "Epoch 88/100\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 0.4158 - acc: 0.8241\n",
      "Epoch 89/100\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 0.4158 - acc: 0.8182\n",
      "Epoch 90/100\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 0.4155 - acc: 0.8261\n",
      "Epoch 91/100\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 0.4162 - acc: 0.8221\n",
      "Epoch 92/100\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 0.4157 - acc: 0.8281\n",
      "Epoch 93/100\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 0.4152 - acc: 0.8261A: 0s - loss: 0.4438 - ac\n",
      "Epoch 94/100\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 0.4152 - acc: 0.8261\n",
      "Epoch 95/100\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 0.4147 - acc: 0.8261\n",
      "Epoch 96/100\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 0.4151 - acc: 0.8241\n",
      "Epoch 97/100\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 0.4148 - acc: 0.8202\n",
      "Epoch 98/100\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 0.4159 - acc: 0.8241\n",
      "Epoch 99/100\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 0.4144 - acc: 0.8241\n",
      "Epoch 100/100\n",
      "506/506 [==============================] - 1s 2ms/step - loss: 0.4146 - acc: 0.8221\n",
      "506/506 [==============================] - 0s 862us/step\n",
      "\n",
      "Loss: 0.41, Accuracy: 82.21%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#---------------Enter Your Code Here---------------------#\n",
    "\n",
    "model = Sequential()\n",
    "# Adds the input layer and 5 neuron hidden layer\n",
    "model.add(Dense(5,input_dim=2, activation='relu'))\n",
    "# Adds an additional 5 neuron hidden layer\n",
    "model.add(Dense(5, activation = 'relu'))\n",
    "# Adds a one neuron output layer\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X_2D, y_mean, epochs=100, batch_size=10)\n",
    "loss, _ = model.evaluate(X_2D, y_mean)\n",
    "probabilities = model.predict(X_2D)\n",
    "predictions = [float(np.round(x)) for x in probabilities]\n",
    "accuracy = np.mean(predictions == y_mean)\n",
    "print(\"\\nLoss: %.2f, Accuracy: %.2f%%\" % (loss, accuracy*100))\n",
    "\n",
    "#--------------------------------------------------------#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced: CNNs and RNNs\n",
    "- Convolutional Neural Networks,CNNs, are a variation of neural networks. They\n",
    "  are the default choice for image classification at present.\n",
    "    - Google famously used CNNs to identify cats via millions of cat images.\n",
    "    - [learn more](https://medium.freecodecamp.org/an-intuitive-guide-to-convolutional-neural-networks-260c2de0a050)\n",
    "- Recurrent Neural Networks, RNNs, are another form of neural network most\n",
    "  commonly used for handwriting, speech recognition, or text segmentation. \n",
    "    - a common task would be computer generated Shakespeare or Siri identifying\n",
    "      a question.\n",
    "    - [learn more](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)\n",
    "\n",
    "## Conclusion\n",
    "We were introduced to a new tool: neural networks. We saw that linear and\n",
    "logistic regression were simple cases of neural networks, and we saw how to use\n",
    "Keras to easily implement a neural network. \n",
    "\n",
    "Neural nets work best when there is a nonlinear relationship to learn and there\n",
    "is a lot of data (millions of rows). The applications are endless and still\n",
    "currently waiting to be found in every industry. It's an exciting time because,\n",
    "with some effort, we can be the first to apply deep learning in a novel,\n",
    "effective way. This is not like physics or math, which have hardly changed for \n",
    "hundreds of years."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
