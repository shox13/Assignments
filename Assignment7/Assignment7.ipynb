{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Neural Networks\n",
    "Neural Networks are another algorithm we can use to learn complicated,\n",
    "nonlinear relations among our data, and they work best with large amounts of \n",
    "data. \n",
    "\n",
    "Every neural net has an input layer, zero or more hidden layers, and one output\n",
    "layer. The input layer accepts the training examples, each hidden layer\n",
    "performs the calculation $\\sigma\\left(w^TX + b\\right)$, and the output layer\n",
    "performs the same calculation one last time. \n",
    "\n",
    "The simplest 1-layer neural net has only an input and output layer.\n",
    "Linear and logistic regression are also instances of 1-layer neural nets\n",
    "because they have an input layer that accepts training examples and an output\n",
    "layer that performs a calculation and outputs a value. \n",
    "\n",
    "## Infrastructure\n",
    "We'll load all libraries and pre-built functions here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def norm_data(X):\n",
    "\n",
    "    scaler = StandardScaler().fit(X)\n",
    "    X_norm = scaler.transform(X)\n",
    "\n",
    "    return X_norm\n",
    "\n",
    "def pca_transform(n, data, inv = False):\n",
    "    pca = PCA(n_components = n, random_state = 0)\n",
    "    data_norm = norm_data(data)\n",
    "\n",
    "    if inv:\n",
    "        Z = pca.inverse_transform(Z)*data.std(axis=0) + data.mean(axis=0)\n",
    "    else:\n",
    "        Z = pca.fit_transform(data_norm)\n",
    "\n",
    "    return Z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear and Logistic Regression as Neural Nets\n",
    "Let's take an algorithm we know-logistic regression-and describe it in terms of \n",
    "a neural net. \n",
    "\n",
    "![Logistic Regression as Neural Net source: deeplearning.ai](log_reg_as_nn.png){width=50%}\n",
    "\n",
    "We've seen previously that logistic regression takes the input examples (xi and\n",
    "yi), learns optimal parameters(wi and b), finds the output $w^TX + b$, \n",
    "squeezes the value into (0,1) via the sigmoid function, and finally\n",
    "classifies each example as 1 (pass) or 0 (fail) based on a typical 0.5\n",
    "probability threshold. \n",
    "\n",
    "Interpreting this as a neural net, the training examples (xi) are the input layer,\n",
    "and the \"neuron\" that computes $\\sigma\\left(w^TX + b\\right)$ is the output layer.\n",
    "In this way, we say logistic regression is also a 1-layer neural network. \n",
    "\n",
    "For linear regression, the interpretation of input and output layers is the\n",
    "same, except $w^TX + b$ is not passed through an activation function like the\n",
    "sigmoid function, $\\sigma$. \n",
    "\n",
    "An activation function changes the nature of the values $w^TX + b$. For\n",
    "example, if we want a probability, we send these values through the sigmoid \n",
    "activation function, which returns a value in (0,1). \n",
    "\n",
    "## Intro to Neural Network\n",
    "\n",
    "![Basic Neural Net source: Stanford CS231n](nn.png){width=50%}\n",
    "\n",
    "The only difference between linear and logistic regression and neural networks\n",
    "is the number of hidden layers. The smallest neural network has one hidden\n",
    "layer. The hidden layer computes $w^TX + b$ and passes it through an activation \n",
    "function. In the simplest case, these values are passed to the ouput layer, \n",
    "where another set of $w^TX + b$ values are computed and passed through another \n",
    "activation function such as the sigmoid function. These values are then\n",
    "classified as 1 (pass) or 0 (fail). \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Linear Classification with Keras\n",
    "Keras is a pre-built library which provides us a simple interface to implement\n",
    "neural networks. Let's try linear classification on the first two principal\n",
    "components of Boston Housing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston = load_boston()\n",
    "X, y = boston.data, boston.target\n",
    "y_mean = np.where(y>y.mean(), 1, 0)\n",
    "X_2D = pca_transform(2, X)\n",
    "\n",
    "# lin. classification in terms of neural net\n",
    "model = Sequential()\n",
    "# this adds the input and output layers\n",
    "# relu is a different choice than sigmoid for activation function\n",
    "model.add(Dense(1, input_dim=2, activation='relu'))\n",
    "# mse (mean squared error) is the loss function used for linear regression\n",
    "# sgd (stochastic gradient descent) is the optimization algorithm used to \n",
    "# find the optimal parameters W\n",
    "model.compile(loss='mse', optimizer='sgd', metrics=['accuracy'])\n",
    "# epochs are the number of iterations of sgd to run\n",
    "# batch size is used to control how many examples are trained at a time\n",
    "model.fit(X_2D, y_mean, epochs=100, batch_size=10)\n",
    "loss, _ = model.evaluate(X_2D, y_mean)\n",
    "# predict returns a probability in (0,1)\n",
    "probabilities = model.predict(X_2D)\n",
    "# this converts probs. to 0 or 1\n",
    "predictions = [float(np.round(x)) for x in probabilities]\n",
    "accuracy = np.mean(predictions == y_mean)\n",
    "print(\"\\nLoss: %.2f, Accuracy: %.2f%%\" % (loss, accuracy*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Classification with Keras\n",
    "Now, let's implement logistic regression in a similar manner:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# log. classification in terms of neural net\n",
    "model = Sequential()\n",
    "model.add(Dense(1, input_dim=2, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X_2D, y_mean, epochs=100, batch_size=10)\n",
    "loss, _ = model.evaluate(X_2D, y_mean)\n",
    "probabilities = model.predict(X_2D)\n",
    "predictions = [float(np.round(x)) for x in probabilities]\n",
    "accuracy = np.mean(predictions == y_mean)\n",
    "print(\"\\nLoss: %.2f, Accuracy: %.2f%%\" % (loss, accuracy*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Net with Keras\n",
    "Time to add some hidden layers and create a neural net!:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# neural net on same data\n",
    "model = Sequential()\n",
    "# this adds the input layer and a 5 neuron hidden layer\n",
    "model.add(Dense(5,input_dim=2, activation='relu'))\n",
    "# this adds the one-neuron output layer\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "# binary crossentropy is the loss used in logistic regression \n",
    "# adam is a substitute for gradient descent to find the optimal parameters, W.\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X_2D, y_mean, epochs=100, batch_size=10)\n",
    "loss, _ = model.evaluate(X_2D, y_mean)\n",
    "probabilities = model.predict(X_2D)\n",
    "predictions = [float(np.round(x)) for x in probabilities]\n",
    "accuracy = np.mean(predictions == y_mean)\n",
    "print(\"\\nLoss: %.2f, Accuracy: %.2f%%\" % (loss, accuracy*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "We've seen how to build a neural net with Keras by adding layers. Now, your\n",
    "task is to build a 3-layer neural net on Boston Housing by additing one \n",
    "additional layer. Use the previous examples as a guide and experiment with the\n",
    "number of neurons in each layer to try to get a better fit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#---------------Enter Your Code Here---------------------#\n",
    "\n",
    "\n",
    "#--------------------------------------------------------#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced: CNNs and RNNs\n",
    "- Convolutional Neural Networks,CNNs, are a variation of neural networks. They\n",
    "  are the default choice for image classification at present.\n",
    "    - Google famously used CNNs to identify cats via millions of cat images.\n",
    "    - [learn more](https://medium.freecodecamp.org/an-intuitive-guide-to-convolutional-neural-networks-260c2de0a050)\n",
    "- Recurrent Neural Networks, RNNs, are another form of neural network most\n",
    "  commonly used for handwriting, speech recognition, or text segmentation. \n",
    "    - a common task would be computer generated Shakespeare or Siri identifying\n",
    "      a question.\n",
    "    - [learn more](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)\n",
    "\n",
    "## Conclusion\n",
    "We were introduced to a new tool: neural networks. We saw that linear and\n",
    "logistic regression were simple cases of neural networks, and we saw how to use\n",
    "Keras to easily implement a neural network. \n",
    "\n",
    "Neural nets work best when there is a nonlinear relationship to learn and there\n",
    "is a lot of data (millions of rows). The applications are endless and still\n",
    "currently waiting to be found in every industry. It's an exciting time because,\n",
    "with some effort, we can be the first to apply deep learning in a novel,\n",
    "effective way. This is not like physics or math, which have hardly changed for \n",
    "hundreds of years."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
