{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Market for AI\n",
    "In the last lesson, we saw models like logistic regression and random forests, which\n",
    "are examples of supervised learning. That is, these models receive labelled\n",
    "data (x, y), and learn relationships in the data to make a prediction. \n",
    "\n",
    "However, the data we collect won't always have a label, and that's\n",
    "where different unsupervised learning techniques can help. \n",
    "\n",
    "## k-means Clustering\n",
    "\n",
    "k-means clustering is a method that partitions a dataset into *k* different groups, with each data \n",
    "point belonging to the cluster with the nearest mean. With k-means clustering, we can segment into any number of groups.\n",
    "\n",
    "For example, say you've developed a tool for Airbnb \n",
    "hosts that automates their guest interactions. You have four different \n",
    "subscription tiers. You'd like to be able to market each tier to different \n",
    "groups, but you are not sure which customer fits in which group. \n",
    "\n",
    "You can run k-means clustering on the data and separate the data different groups. You would like to see the results graphically, but the data has many feautures (including the number of properties managed, size of each property, and\n",
    "location), and we can only graph something in three dimensions (x, y, and z axes). \n",
    "\n",
    "## Principal Component Analysis\n",
    "\n",
    "Principal component analysis (PCA) reduces the number of features \n",
    "under consideration and helps focus the analysis on the so-called \n",
    "\"principal component\" affecting the behavior of the data. With a tool like \n",
    "PCA, we can reduce the dimensionality of the data to a viewable form. \n",
    "\n",
    "Let's take another look at the Boston housing data to get a feel for these tools.\n",
    "\n",
    "## Step 1: Load the Infrastructure\n",
    "Run the following cell to load all the functions that we will need later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits import mplot3d\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import seaborn as sns; sns.set()\n",
    "\n",
    "def norm_data(X):\n",
    "    mu = X.mean(axis=0)\n",
    "    sd = X.std(axis=0)\n",
    "    X_norm = (X - mu) / sd\n",
    "\n",
    "    return X_norm\n",
    "\n",
    "def pca_transform(n, data, inv = False):\n",
    "    pca = PCA(n_components = n, random_state = 0)\n",
    "    data_norm = norm_data(data)\n",
    "    Z = pca.fit_transform(data_norm)\n",
    "    if inv:\n",
    "        Z = pca.inverse_transform(Z)*data.std(axis=0) + data.mean(axis=0)\n",
    "\n",
    "    return Z\n",
    "\n",
    "def plot_PCs(n, data, y, km = False):\n",
    "\n",
    "    Z = pca_transform(n, data)\n",
    "\n",
    "    if km:\n",
    "        kmeans = KMeans(4)\n",
    "        kmeans.fit(Z)\n",
    "        cl = kmeans.labels_\n",
    "\n",
    "    # Split data by MEDV > mean.\n",
    "    gt = Z[y==1]\n",
    "    lt = Z[y==0]\n",
    "\n",
    "    # 3D k-means.\n",
    "    if km and n == 3:\n",
    "        ax = plt.subplot(projection='3d')\n",
    "        ax.scatter3D(Z[:,0], Z[:,1], Z[:,2], c=cl,  cmap = \"Accent\")\n",
    "\n",
    "        ax = plt.gca()\n",
    "        ax.set_zlabel(\"3rd PC\")\n",
    "        plt.xlabel(\"1st PC\")\n",
    "        plt.ylabel(\"2nd PC\")\n",
    "\n",
    "\n",
    "    # 3D MEDV split.\n",
    "    elif n == 3:\n",
    "        ax = plt.subplot(projection='3d')\n",
    "        ax.scatter3D(gt[:,0], gt[:,1], gt[:,2], marker = '+', cmap = \"Accent\",\n",
    "                    label = \"Greater than mean MEDV\")\n",
    "        ax.scatter3D(lt[:,0], lt[:,1], lt[:,2], s =5, marker = 'o', cmap = \"Accent\",\n",
    "                    label = \"Less than mean MEDV\")\n",
    "\n",
    "        ax = plt.gca()\n",
    "        ax.set_zlabel(\"3rd PC\")\n",
    "        plt.xlabel(\"1st PC\")\n",
    "        plt.ylabel(\"2nd PC\")\n",
    "\n",
    "    # 2D k-means.\n",
    "    if km and n == 2:\n",
    "        plt.scatter(Z[:,0], Z[:,1], c=cl,  cmap = \"Accent\")\n",
    "\n",
    "        plt.xlabel(\"1st PC\")\n",
    "        plt.ylabel(\"2nd PC\")\n",
    "    # 2D MEDV split.\n",
    "    elif n == 2:\n",
    "        plt.scatter(gt[:,0], gt[:,1],  marker = '+', cmap = \"Accent\",\n",
    "                    label = \"Greater than mean MEDV\")\n",
    "        plt.scatter(lt[:,0], lt[:,1], s=5, marker = 'o', cmap = \"Accent\",\n",
    "                    label = \"Less than mean MEDV\")\n",
    "        plt.xlabel(\"1st PC\")\n",
    "        plt.ylabel(\"2nd PC\")\n",
    "\n",
    "    # 1D k-means.\n",
    "    if km and n == 1:\n",
    "        plt.scatter(Z[:,0],[0]*Z.shape[0], s=5, c=cl, cmap = \"Accent\")\n",
    "\n",
    "        plt.xlabel(\"1st PC\")\n",
    "        plt.title(\"Boston Housing: PCA Reduced\")\n",
    "        ax = plt.gca()\n",
    "        ax.set(yticks=[])\n",
    "    # 1D MEDV split.\n",
    "    elif n == 1:\n",
    "        plt.scatter(gt[:,0],[0]*gt.shape[0], marker = '+', cmap = \"Accent\",\n",
    "                    label = \"Greater than mean MEDV\")\n",
    "        plt.scatter(lt[:,0],[0]*lt.shape[0], s=5, marker = 'o', cmap = \"Accent\",\n",
    "                    label = \"Less than mean MEDV\")\n",
    "\n",
    "        plt.xlabel(\"1st PC\")\n",
    "        plt.title(\"Boston Housing: PCA Reduced\")\n",
    "        ax = plt.gca()\n",
    "        ax.set(yticks=[])\n",
    "\n",
    "    if not km:\n",
    "        plt.legend()\n",
    "    if km:\n",
    "        plt.title(\"Boston Housing: PCA and K-Means\")\n",
    "    else:\n",
    "        plt.title(\"Boston Housing: PCA Reduced\")\n",
    "    plt.show()\n",
    "\n",
    "def plot_diff_means(d1, d2, labels):\n",
    "    fig, ax = plt.subplots()\n",
    "    bar_width = 0.35\n",
    "    opacity = 0.4\n",
    "    index = np.arange(d1.shape[1])\n",
    "    mean1 = d1.mean(axis=0)\n",
    "    mean2 = d2.mean(axis=0)\n",
    "    diff = 100*(mean1 - mean2) / mean2\n",
    "    \n",
    "    rect1 = ax.bar(index, diff, bar_width,\n",
    "                alpha=opacity, color='b',\n",
    "                label = \"% Difference\")\n",
    "\n",
    "    plt.xticks(index, labels, rotation = 45)\n",
    "    plt.ylabel(\"Percentage\")\n",
    "    plt.title(\"Difference of Means for Of Interest and Original Data\")\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Run a Principal Component Analysis\n",
    "\n",
    "First, we will reduce the dimensionality and take a look at the homes with values that are greater than and\n",
    "less than the mean home value (MEDV). Run the following cell to see the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_boston().data\n",
    "target = load_boston().target\n",
    "y = np.zeros_like(target)\n",
    "y[target > target.mean()] = 1\n",
    "\n",
    "for i in range(1,4):\n",
    "    plot_PCs(i, data, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "\n",
    "Originally, the data contained 506 examples with 13 different features and it\n",
    "wasn't possible to visualize. With the help of PCA, we can reduce our dataset\n",
    "down to the three, two, or one most important features. This makes\n",
    "visualization possible. \n",
    "\n",
    "It's worth taking another look at the graph with one principal component. There\n",
    "are regions on the edges where home values are mostly greater than or less than MEDV.\n",
    "By investigating what values of our original 13 features connect to this data, \n",
    "we might find a way to determine why these home values are so different.\n",
    "\n",
    "## Step 3: Run a Second Principal Component Analysis\n",
    "\n",
    "Run the following cell to see the features that affect impact the MEDV outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to pick out interesting data.\n",
    "Z = pca_transform(1, data)\n",
    "# Of all homes < mean MEDV, what is the least? \n",
    "min_lt = Z[y==0].min()\n",
    "ind = []\n",
    "for i, v in enumerate(Z):\n",
    "    # Grab homes that are to the left of min_lt on the PC1 graph.\n",
    "    if v < min_lt:\n",
    "        ind.append(i)\n",
    "\n",
    "# Go back to 13 features.\n",
    "Z_rec = pca_transform(1, data, inv=True)\n",
    "# Grab points of interest in the original 13 feature space.\n",
    "Z_rec[ind]\n",
    "bar_labels = load_boston().feature_names\n",
    "# Plot the difference between the data as a whole and our points of interest.\n",
    "plot_diff_means(Z_rec[ind], data, bar_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CRIM and ZN immediately stand out.\n",
    "\n",
    "- CRIM is the per capita crime rate \n",
    "- ZN is the proportion of residential land zoned for lots over 25,000 sq. ft. (2,323 sq. m.).\n",
    "\n",
    "CRIM is much lower and ZN is much bigger. This is probably an exclusive\n",
    "neighborhood filled with large homesâ€”a 25,000 sq. ft. (2,323 sq. m.) lot is pretty big. \n",
    "\n",
    "By looking at the graph of the first principal component, we see that there are\n",
    "two regions that stand out from the data: the region < -4 and the region > 4.\n",
    "\n",
    "We took these data points < -4 and transformed them back into the original 13\n",
    "feature space. After comparing the means of our data of interest with the\n",
    "original data, we saw that CRIM and ZN were very different, leading us to the\n",
    "conclusion that our data of interest probably represents an exclusive\n",
    "neighborhood filled with large homes. \n",
    "\n",
    "In the following exercise, you will repeat this process for values from the first principal component > 4. \n",
    "\n",
    "## Step 4: Run Another PCA\n",
    "\n",
    "In step 3, we discovered that two regions stood out from the data: < -4 and > 4. We then transformed the values < -4 from the first principal component. \n",
    "\n",
    "\n",
    "In this step, you will add values to the following code cell to transform the values > 4 from the first principal component. \n",
    "\n",
    "**Note**: To see the code that we used, see **Answer Code** below the code cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to investigate the opposite side of the first principal component.\n",
    "Z = pca_transform(1, data)\n",
    "# Of all homes < mean MEDV, what is the least? \n",
    "max_gt = \n",
    "ind = []\n",
    "Z_rec = \n",
    "plot_diff_means(Z_rec[ind], data, bar_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer Code\n",
    "We used the following code in the code cell.\n",
    "\n",
    "```python\n",
    "Z = pca_transform(1, data)\n",
    "# Of all homes < mean MEDV, what is the least? \n",
    "max_gt = Z[y==1].max()\n",
    "ind = []\n",
    "for i, v in enumerate(Z):\n",
    "    # Grab homes that are to the right of max_gt on the PC1 graph.\n",
    "    if v > max_gt:\n",
    "        ind.append(i)\n",
    "\n",
    "# Go back to 13 features.\n",
    "Z_rec = pca_transform(1, data, inv=True)\n",
    "# Grab points of interest in the original 13 feature space.\n",
    "Z_rec[ind]\n",
    "bar_labels = load_boston().feature_names\n",
    "# Plot the difference between the data as a whole and our points of interest.\n",
    "plot_diff_means(Z_rec[ind], data, bar_labels)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: k-means Clustering \n",
    "\n",
    "Now that we're able to visualize our data, let's segment it further. Let's say\n",
    "you're selling four different versions of a product to families in the Boston\n",
    "area. You'd like to segment your potential customers into four distinct groups,\n",
    "so you can better target your advertising to them. \n",
    "\n",
    "Run the following cell to perform k-means clustering on the PCA-reduced Boston housing data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,4):\n",
    "    plot_PCs(i, data, y, km = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could then follow the same process to back out what our groups look like in\n",
    "our original data to come up with profiles on them. \n",
    "\n",
    "k-means clustering is a powerful tool that can help you group your data better. \n",
    "Without any labels, k-means clustering is able to group the housing data into any number\n",
    "of clusters. This can be very useful, especially for customer segmentation. \n",
    "\n",
    "## Additional Exercise \n",
    "Clustering or segmenting a dataset into different groups is valuable for a\n",
    "number of cases; for example, trying to target customers for specific products\n",
    "within a product offering. For further practice:\n",
    "- Investigate how the different clusters correspond to the original 13\n",
    "  features by following the same procedure from the PCA from earlier. \n",
    "    - Isolate the principal component values that correspond to each group.\n",
    "    - Back out your highlighted data to the original 13 feature space.\n",
    "    - Compare the means of the highlighted data to those of the original data.\n",
    "\n",
    "\n",
    "## Additional Reading\n",
    "For more information about the math in this coding exercise, see the following\n",
    "resources:\n",
    "\n",
    "- [Andrew Ng on K-Means](https://www.youtube.com/watch?v=Ev8YbxPu_bQ)\n",
    "- [PCA Tutorial](http://www.cs.otago.ac.nz/cosc453/student_tutorials/principal_components.pdf)\n",
    "\n",
    "## Conclusion\n",
    "We've seen two new tools from unsupervised learning (learning on unlabeled\n",
    "data): k-means clustering and PCA. k-means clustering was able to find unique labels for our data\n",
    "based on the number of groups we wantd. PCA reduced the dimensionality of our\n",
    "data, making it possible to visualize. These are both effective tools because\n",
    "most of the data \"in the wild\" doesn't come with labels. PCA and\n",
    "k-means add meaning to this data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
