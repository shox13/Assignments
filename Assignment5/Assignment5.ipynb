{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Market for AI\n",
    "Previously, we saw models like Logistic Regression and Random Forests, which\n",
    "were examples of supervised learning. That is, these models were fed labelled\n",
    "data (X, y), then learned a relationship among the data in order to make a \n",
    "prediction. Yet, the data we collect won't always have a label, and that's\n",
    "where different unsupervised learning techniques can help. \n",
    "\n",
    "For example, take customer segmentation. Say you've developed a tool for Airbnb \n",
    "hosts that automates their guest interactions and you have four different \n",
    "subscription tiers. You'd like to be able to market each tier to different \n",
    "groups, but your not sure which customer fits in which group. K-means clustering \n",
    "is a method  that partitions a dataset into k different groups, with each data \n",
    "point belonging to the cluster with the nearest mean. With it, we can take our \n",
    "data and segment it into any number of groups we want.\n",
    "\n",
    "So, we run k-means on our data and seperate different groups, but now we would \n",
    "like to get a sense for what's happening by graphing it. The problem is the \n",
    "data has many feautures (number of properties managed, size of each property,\n",
    "location), and we can only graph something in three dimensions (x, y, and z \n",
    "axes). Principal Component Analysis is meant to reduce the number of features \n",
    "under consideration and to help focus the analysis on the so-called \n",
    "\"principal component\" affecting the behavior of the data. With a tool like \n",
    "Principal Component Analysis (PCA), we can reduce the dimensionality of our \n",
    "data to a viewable form. Let's take a look at our Boston housing data to get a \n",
    "feel for these tools.\n",
    "\n",
    "## Infrastructure\n",
    "We'll load all the functions that we'll be using later here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits import mplot3d\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import seaborn as sns; sns.set()\n",
    "\n",
    "def norm_data(X):\n",
    "    mu = X.mean(axis=0)\n",
    "    sd = X.std(axis=0)\n",
    "    X_norm = (X - mu) / sd\n",
    "\n",
    "    return X_norm\n",
    "\n",
    "def pca_transform(n, data, inv = False):\n",
    "    pca = PCA(n_components = n, random_state = 0)\n",
    "    data_norm = norm_data(data)\n",
    "    Z = pca.fit_transform(data_norm)\n",
    "    if inv:\n",
    "        Z = pca.inverse_transform(Z)*data.std(axis=0) + data.mean(axis=0)\n",
    "\n",
    "    return Z\n",
    "\n",
    "def plot_PCs(n, data, y, km = False):\n",
    "\n",
    "    Z = pca_transform(n, data)\n",
    "\n",
    "    if km:\n",
    "        kmeans = KMeans(4)\n",
    "        kmeans.fit(Z)\n",
    "        cl = kmeans.labels_\n",
    "\n",
    "    # split data by MEDV > mean\n",
    "    gt = Z[y==1]\n",
    "    lt = Z[y==0]\n",
    "\n",
    "    # 3D K-Means\n",
    "    if km and n == 3:\n",
    "        ax = plt.subplot(projection='3d')\n",
    "        ax.scatter3D(Z[:,0], Z[:,1], Z[:,2], c=cl,  cmap = \"Accent\")\n",
    "\n",
    "        ax = plt.gca()\n",
    "        ax.set_zlabel(\"3rd PC\")\n",
    "        plt.xlabel(\"1st PC\")\n",
    "        plt.ylabel(\"2nd PC\")\n",
    "\n",
    "\n",
    "    # 3D MEDV split\n",
    "    elif n == 3:\n",
    "        ax = plt.subplot(projection='3d')\n",
    "        ax.scatter3D(gt[:,0], gt[:,1], gt[:,2], marker = '+', cmap = \"Accent\",\n",
    "                    label = \"Greater than mean MEDV\")\n",
    "        ax.scatter3D(lt[:,0], lt[:,1], lt[:,2], s =5, marker = 'o', cmap = \"Accent\",\n",
    "                    label = \"Less than mean MEDV\")\n",
    "\n",
    "        ax = plt.gca()\n",
    "        ax.set_zlabel(\"3rd PC\")\n",
    "        plt.xlabel(\"1st PC\")\n",
    "        plt.ylabel(\"2nd PC\")\n",
    "\n",
    "    # 2D K-Means\n",
    "    if km and n == 2:\n",
    "        plt.scatter(Z[:,0], Z[:,1], c=cl,  cmap = \"Accent\")\n",
    "\n",
    "        plt.xlabel(\"1st PC\")\n",
    "        plt.ylabel(\"2nd PC\")\n",
    "    # 2D MEDV split\n",
    "    elif n == 2:\n",
    "        plt.scatter(gt[:,0], gt[:,1],  marker = '+', cmap = \"Accent\",\n",
    "                    label = \"Greater than mean MEDV\")\n",
    "        plt.scatter(lt[:,0], lt[:,1], s=5, marker = 'o', cmap = \"Accent\",\n",
    "                    label = \"Less than mean MEDV\")\n",
    "        plt.xlabel(\"1st PC\")\n",
    "        plt.ylabel(\"2nd PC\")\n",
    "\n",
    "    # 1D K-Means\n",
    "    if km and n == 1:\n",
    "        plt.scatter(Z[:,0],[0]*Z.shape[0], s=5, c=cl, cmap = \"Accent\")\n",
    "\n",
    "        plt.xlabel(\"1st PC\")\n",
    "        plt.title(\"Boston Housing: PCA Reduced\")\n",
    "        ax = plt.gca()\n",
    "        ax.set(yticks=[])\n",
    "    # 1D MEDV split\n",
    "    elif n == 1:\n",
    "        plt.scatter(gt[:,0],[0]*gt.shape[0], marker = '+', cmap = \"Accent\",\n",
    "                    label = \"Greater than mean MEDV\")\n",
    "        plt.scatter(lt[:,0],[0]*lt.shape[0], s=5, marker = 'o', cmap = \"Accent\",\n",
    "                    label = \"Less than mean MEDV\")\n",
    "\n",
    "        plt.xlabel(\"1st PC\")\n",
    "        plt.title(\"Boston Housing: PCA Reduced\")\n",
    "        ax = plt.gca()\n",
    "        ax.set(yticks=[])\n",
    "\n",
    "    if not km:\n",
    "        plt.legend()\n",
    "    if km:\n",
    "        plt.title(\"Boston Housing: PCA and K-Means\")\n",
    "    else:\n",
    "        plt.title(\"Boston Housing: PCA Reduced\")\n",
    "    plt.show()\n",
    "\n",
    "def plot_diff_means(d1, d2, labels):\n",
    "    fig, ax = plt.subplots()\n",
    "    bar_width = 0.35\n",
    "    opacity = 0.4\n",
    "    index = np.arange(d1.shape[1])\n",
    "    mean1 = d1.mean(axis=0)\n",
    "    mean2 = d2.mean(axis=0)\n",
    "    diff = 100*(mean1 - mean2) / mean2\n",
    "    \n",
    "    rect1 = ax.bar(index, diff, bar_width,\n",
    "                alpha=opacity, color='b',\n",
    "                label = \"% Difference\")\n",
    "\n",
    "    plt.xticks(index, labels, rotation = 45)\n",
    "    plt.ylabel(\"Percentage\")\n",
    "    plt.title(\"Difference of Means for Of Interest and Original Data\")\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal Component Analysis\n",
    "First, we'll reduce the dimensionality and take a look at the homes greater and\n",
    "less than the mean home value (MEDV):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_boston().data\n",
    "target = load_boston().target\n",
    "y = np.zeros_like(target)\n",
    "y[target > target.mean()] = 1\n",
    "\n",
    "for i in range(1,4):\n",
    "    plot_PCs(i, data, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Originally, our data contained 506 examples with 13 different features and it\n",
    "wasn't possible to visualize. With the help of PCA, we can reduce our dataset\n",
    "down to the three, two, or one most important features. And, this makes\n",
    "visualization possible. \n",
    "\n",
    "It's worth taking another look at the graph with one principal component. There\n",
    "are regions on the edges where homes are mostly greater than or less than MEDV.\n",
    "By investigating what values of our original 13 features connect to this data, \n",
    "we might find a way to pick out these high and low value homes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code to pick out interesting data\n",
    "Z = pca_transform(1, data)\n",
    "# of all homes < mean MEDV, what is the least \n",
    "min_lt = Z[y==0].min()\n",
    "ind = []\n",
    "for i, v in enumerate(Z):\n",
    "    # grab homes that are to the left of min_lt on PC1 graph\n",
    "    if v < min_lt:\n",
    "        ind.append(i)\n",
    "\n",
    "# go back to 13 features\n",
    "Z_rec = pca_transform(1, data, inv=True)\n",
    "# grab points of interest in original 13 feature space\n",
    "Z_rec[ind]\n",
    "bar_labels = load_boston().feature_names\n",
    "# plot diff. between data as a whole and our points of interest\n",
    "plot_diff_means(Z_rec[ind], data, bar_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CRIM and ZN immediately pop out. the variables are:\n",
    "- CRIM is the per capita crime rate and \n",
    "- ZN is the proportion of residential land zoned for lots over 25,000 sq. ft.\n",
    "\n",
    "CRIM is much lower and ZN is much bigger. This is probably an exclusive\n",
    "neighborhood filled with large homes-25,000 sq. ft is pretty big. \n",
    "\n",
    "\n",
    "By looking at the graph of the 1st Principal Component, we saw that there were\n",
    "two regions that stood out from the data: the region < -4 and the region > 4.\n",
    "We grabbed the data points < -4 and transformed them back to the original 13\n",
    "feature space. After comparing the means of our data of interest with the\n",
    "original data, we saw that CRIM and ZN were much different, leading us to the\n",
    "conclusion that our data of interest probably represents an exclusive\n",
    "neighborhood filled with large homes. In the following exercise, you will get\n",
    "the chance to repeat this process for values from the 1st Principal component > 4. \n",
    "\n",
    "## Exercise\n",
    "We saw how to back out the values < -4 from the 1st Principal Component. Now,\n",
    "repeat the process for values > 4 from the 1st Principal Component. Reference\n",
    "the graph of the 1st Principal Component above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code to investigate the opposite side of the 1st prin. comp.\n",
    "Z = pca_transform(1, data)\n",
    "# of all homes < mean MEDV, what is the least \n",
    "max_gt = \n",
    "ind = []\n",
    "Z_rec = \n",
    "plot_diff_means(Z_rec[ind], data, bar_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Means Clustering \n",
    "Now that we're able to visualize our data, let's segment it further. Let's say\n",
    "you're selling four different versions of a product to families in the Boston\n",
    "area. You'd like to segment your potential customers into four distinct groups,\n",
    "so you can better target your advertising to them. Let's perform k-means on\n",
    "the PCA-reduced Boston Housing data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,4):\n",
    "    plot_PCs(i, data, y, km = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could then follow the same process to back out what our groups looks like in\n",
    "our original data to come up with profiles on them. \n",
    "\n",
    "K-means clustering is a powerful tool that can help you group your data better. \n",
    "Without any labels, k-means is able to group the housing data into any number\n",
    "of clusters. This can be very useful especially for customer segmentation. \n",
    "\n",
    "## Additional Exercise \n",
    "Clustering or segmenting a dataset into different groups is valuable for a\n",
    "number of cases; for example, trying to target customers for specific products\n",
    "within a product offering. For further practice:\n",
    "- Investigate how the different clusters correspond to the original 13\n",
    "  features by following the same procedure from PCA from earlier. \n",
    "    - isolate what principal component values correspond to each group\n",
    "    - back out your highlighted data to the original 13 feature space\n",
    "    - compare the means of highlighted data to those of the original data\n",
    "\n",
    "\n",
    "## Additional Reading\n",
    "If you're interested in digging in deeper to the math, here are some helpful\n",
    "resources:\n",
    "\n",
    "- [Andrew Ng on K-Means](https://www.youtube.com/watch?v=Ev8YbxPu_bQ)\n",
    "- [PCA Tutorial](http://www.cs.otago.ac.nz/cosc453/student_tutorials/principal_components.pdf)\n",
    "\n",
    "## Conclusion\n",
    "We've seen two new tools from unsupervised learning (learning on unlabeled\n",
    "data): K-Means and PCA. K-means was able to find unique labels for our data\n",
    "based on the number of groups we wantd. PCA reduced the dimensionality of our\n",
    "data, making it possible to visualize. These are both effective tools because\n",
    "most of the data in the wild doesn't come with a label at first. PCA and\n",
    "K-Means are able add meaning to this data."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
