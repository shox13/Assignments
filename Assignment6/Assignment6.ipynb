{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI For Competitive Advantage\n",
    "Today we'll explore Support Vector Machines (SVM), which are a form of supervised\n",
    "learning that produce classification boundaries like logistic regression or\n",
    "random forests. It's another tool in the tool belt when we want to classify\n",
    "our data. More specifically, we'll learn about two characteristics of\n",
    "SVMs: the hyperplane and maximal margin classifier. And, we'll learn how to\n",
    "implement a SVM with our data. \n",
    "\n",
    "## Infrastructure\n",
    "We'll load libraries and pre-built functions for later use here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.datasets.samples_generator import make_blobs\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns; sns.set()\n",
    "%matplotlib inline\n",
    "\n",
    "def norm_data(X):\n",
    "\n",
    "    scaler = StandardScaler().fit(X)\n",
    "    X_norm = scaler.transform(X)\n",
    "\n",
    "    return X_norm\n",
    "def pca_transform(n, data, inv = False):\n",
    "    pca = PCA(n_components = n, random_state = 0)\n",
    "    data_norm = norm_data(data)\n",
    "\n",
    "    if inv:\n",
    "        Z = pca.inverse_transform(Z)*data.std(axis=0) + data.mean(axis=0)\n",
    "    else:\n",
    "        Z = pca.fit_transform(data_norm)\n",
    "\n",
    "    return Z\n",
    "def visualizeBoundary(X, y, model, plot_name, plot_num=None, \n",
    "                      plot_support=True, toy=False):\n",
    "\n",
    "    if plot_num != None:\n",
    "        plt.subplot(1,3, plot_num)\n",
    "\n",
    "    # Make classification predictions over a grid of values\n",
    "    x1plot = np.linspace(min(X[:,0]), max(X[:,0]), 100)\n",
    "    x2plot = np.linspace(min(X[:,1]), max(X[:,1]), 100)\n",
    "    X1, X2 = np.meshgrid(x1plot, x2plot)\n",
    "    x12 = np.vstack([X1.ravel(), X2.ravel()]).T\n",
    "    #vals = model.predict(x12).reshape(X1.shape)\n",
    "    if isinstance(model, SVC):\n",
    "        vals = model.decision_function(x12).reshape(X1.shape)\n",
    "    else:\n",
    "        vals = model.predict(x12).reshape(X1.shape)\n",
    "\n",
    "    # Plot the SVM boundary\n",
    "    pos =np.where(y == 1)[0] \n",
    "    neg =np.where(y == 0)[0] \n",
    "\n",
    "    plt.scatter(X[pos, 0], X[pos, 1], marker='+',linewidth= 1, \n",
    "                label = 'greater than MEDV') \n",
    "    plt.scatter(X[neg, 0], X[neg, 1], marker='.',linewidth= 1, \n",
    "                label = 'less than MEDV') \n",
    "    if plot_num != None:\n",
    "        plt.contour(X1, X2, vals, colors = 'k',\n",
    "                levels = [0], alpha=0.5)\n",
    "                #levels = [-1, 0, 1], alpha=0.5,\n",
    "                #linestyles=['--', '-', '--'])\n",
    "    else:\n",
    "        plt.contour(X1, X2, vals, colors = 'k',\n",
    "                levels = [-1, 0, 1], alpha=0.5,\n",
    "                linestyles=['--', '-', '--'])\n",
    "\n",
    "    if plot_support and isinstance(model, SVC):\n",
    "        plt.scatter(model.support_vectors_[:, 0],\n",
    "                    model.support_vectors_[:, 1],\n",
    "                    s=300, linewidth=1, facecolors='none',\n",
    "                    edgecolors='b')\n",
    "    fig = plt.gcf()\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.title(plot_name)\n",
    "    if not toy:\n",
    "        plt.xlabel('PC1')\n",
    "        plt.ylabel('PC2')\n",
    "    plt.subplots_adjust(top=0.88, bottom=0.08, left=0.10, right=0.95, \n",
    "                        hspace=0.25, wspace=0.35)\n",
    "    if (plot_num == 1 or plot_num == None) and not toy:\n",
    "        plt.legend(prop={'size': 6})\n",
    "\n",
    "    if plot_num != None:\n",
    "        plt.text(-5,-3.2,'Accuracy = %.2f' % model.score(X,y))\n",
    "        plt.suptitle('Boston Housing: Different Classifiers')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperplane\n",
    "A hyperplane is a subspace whose dimension is one less than that of its ambient\n",
    "space. Let's define what the hyperplane would be in different ambient spaces:\n",
    "\n",
    "- A line: the hyperplane would be a point\n",
    "- A 2D space (think (x,y) plane): the hyperplane would be a line\n",
    "    - every decision boundary we've graphed as a line previously could also be\n",
    "      called a hyperplane\n",
    "- A 3D space (think our graph of 3 Principal Components for Boston Housing):\n",
    "  the hyperplane would be a 2D plane. \n",
    "\n",
    "There are hyperplanes in higher dimensions, but we won't see them in practice\n",
    "because we can't visualize anything beyond 3D spaces. \n",
    "\n",
    "## Support Vector Machine\n",
    "We'll next learn about the fundamentals of the SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# toy example no classifier\n",
    "X, y = make_blobs(n_samples=340, centers = 2,\n",
    "                  random_state=2, cluster_std=1.3)\n",
    "pos = y==1; neg = y==0\n",
    "plt.title('Toy Example')\n",
    "plt.scatter(X[pos, 0], X[pos,1], marker='+')\n",
    "plt.scatter(X[neg, 0], X[neg,1], marker='.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many hyperplanes (lines in this case) you could draw that would \n",
    "seperate the two groups of data. For each line chosen, there is a margin\n",
    "between it and the nearest point from each group. SVM chooses the line that \n",
    "maximizes these distances, which is why it's called a maximum margin\n",
    "classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# toy example with classifier\n",
    "X, y = make_blobs(n_samples=340, centers = 2,\n",
    "                  random_state=2, cluster_std=1.3)\n",
    "clf = SVC(kernel='linear')\n",
    "clf.fit(X, y)\n",
    "ax = plt.gca()\n",
    "ax.annotate('Maximal Margin', xy=(-4.2, -5), xytext=(-4, -2.5),\n",
    "            arrowprops=dict(facecolor='black', shrink=0.05))\n",
    "visualizeBoundary(X,y, clf, 'Toy Example', toy=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The solid line is the decision boundary found by SVM. The distance between the\n",
    "dotted lines is the maximal margin found by the SVM. And, the circles are the \n",
    "support vectors, which are used to find the boundary that has the greatest\n",
    "distance seperating the nearest points from the decision boundary, the maximal\n",
    "margin.\n",
    "\n",
    "When we used linear and logistic regression, the distance from each point to\n",
    "the decision boundary was important. So, if there was an outlier far different\n",
    "from the other points, it had a large effect on our resulting decision\n",
    "boundary.\n",
    "\n",
    "In opposition, A SVM doesn't take into account all data points when drawing a \n",
    "boundary like we saw with linear or logistic regression. It only considers the \n",
    "location of the support vectors. The result is SVM is much less sensitive to \n",
    "outlier data compared to other methods, which is its  strength. \n",
    "\n",
    "To make this more concrete, say our Boston Housing data had the CEO of GE's \n",
    "home included. His home is very expensive, so if we were looking for a decision\n",
    "boundary between homes greater and less than the median (MEDV), this outlier\n",
    "would pull the decision boundaries of linear and logistic regression toward it\n",
    "causing a distortion. In contrast, the CEO's home wouldn't cause the same\n",
    "distortion with a SVM because it only uses the support vectors, in the center\n",
    "of the data, to create a boundary. This makes the SVM boundary more accurate\n",
    "compared to linear or logistic regression in the presence of outliers.\n",
    "\n",
    "## Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# toy example relaxing maximum margins with varying C parameter\n",
    "for C in [1e6, .1]:\n",
    "    clf = SVC(C=C, kernel='linear')\n",
    "    clf.fit(X, y)\n",
    "    title = 'Toy Example: C = {}'.format(C)\n",
    "    visualizeBoundary(X,y, clf, title, toy=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The C parameter has a similar function in SVM as the regularization parameter,\n",
    "lambda, in linear and logistic regression. We can control how strict the maximum margins are through the C parameter of\n",
    "the SVM. The larger C is the stricter the dotted line boundaries. Likewise, the\n",
    "smaller C is the looser the dotted line boundaries become.\n",
    "\n",
    "## Exercise: Boston Housing\n",
    "Let's see how this new method works with the Boston Housing data and other\n",
    "methods we've previously used. Graph the SVM boundary on the first two\n",
    "principal components of the Boston Housing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# graph of SVM on 2D Boston Housing data\n",
    "\n",
    "boston = load_boston()\n",
    "X = boston.data\n",
    "y = boston.target\n",
    "# greater or less than mean for target variable\n",
    "y_mean = np.where(y > y.mean(), 1, 0)\n",
    "#------------------Enter Your Code Here-----------------#\n",
    "X_2D = pca_transform()\n",
    "clf = \n",
    "clf.fit()\n",
    "title = \"Boston Housing with SVM\"\n",
    "visualizeBoundary(X_2D, y_mean, clf, title, plot_support=False)\n",
    "#-------------------------------------------------------#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "We saw that the SVM uses the support vectors to create a seperating hyperplane\n",
    "with maximal margins around the seperated groups. The advantages of SVM are:\n",
    "\n",
    "- They only rely on the support vectors to come up with a classification. This\n",
    "  makes them more resistant to outliers\n",
    "- They can classify data with nonlinear boundaries if needed. \n",
    "\n",
    "The disadvtantages of SVMs are:\n",
    "\n",
    "- You need to carefully choose the C parameter, which can be expensive to find\n",
    "  for larger datasets.\n",
    "- You can't interpret the results as a probability like logistic regression.\n",
    "\n",
    "In sum, it's best to leave SVM to more complicated situations after simpler\n",
    "approaches are exhausted."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
