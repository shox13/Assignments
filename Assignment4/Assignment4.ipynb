{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifying Data Using Decision Trees\n",
    "Decision trees and random forests predict a target based on training\n",
    "examples, similar to linear/logistic regression. But, they go about it in a different\n",
    "manner. We'll explore this alternate approach with an aim of comparing to linear/\n",
    "logistic regression. We'll help our friend the real estate agent make predictions \n",
    "for her clients and compare how these new methods compare to what we've seen\n",
    "previously.\n",
    "\n",
    "\n",
    "## Infrastructure\n",
    "We'll load all the libraries and prebuilt functions here for later use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "from sklearn.datasets import load_boston, make_blobs\n",
    "from sklearn.ensemble import (RandomForestRegressor, RandomForestClassifier,\n",
    "                              BaggingRegressor, BaggingClassifier,\n",
    "                              GradientBoostingClassifier,\n",
    "                              GradientBoostingRegressor)\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import scikitplot as skp\n",
    "%matplotlib inline\n",
    "\n",
    "def plot_regr(X,y,regr, title, size = 10):\n",
    "    \n",
    "    regr.fit(X, y)\n",
    "\n",
    "    y_pred = regr.predict(X)\n",
    "    plt.clf()\n",
    "    plt.plot(y, y_pred, 'o')\n",
    "\n",
    "    perfect_fit = np.linspace(y.min(),y.max(), 20)\n",
    "    plt.plot(perfect_fit, perfect_fit, 'b--', label='Perfect Fit')\n",
    "\n",
    "    \n",
    "    plt.text(5, 40, \"$R^2$ = %.2f\" % regr.score(X,y))\n",
    "    plt.title(title, size = size)\n",
    "    plt.xlabel('True MEDV ($1,000s)')\n",
    "    plt.ylabel('Predicted MEDV ($1,000s)')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def plot_roc(X,y,regr, title, size = 10):\n",
    "    \n",
    "    regr.fit(X, y)\n",
    "\n",
    "    y_pred = regr.predict_proba(X)\n",
    "    text = \"Accuracy = %.2f\" % regr.score(X, y) \n",
    "    plt.text(.5, .5, text)  \n",
    "    skplt.plot_roc_curve(y, y_pred, title = title)\n",
    "    plt.show()\n",
    "\n",
    "def classify(y, threshold):\n",
    "    y_clf = []\n",
    "    for val in y:\n",
    "        if val > threshold:\n",
    "            y_clf.append(1)\n",
    "        else:\n",
    "            y_clf.append(0)\n",
    "\n",
    "    return np.array(y_clf)\n",
    "\n",
    "def visualize_classifier(model, X1, y1, X2=None, y2=None, \n",
    "                         ax=None, cmap='rainbow', title=''):\n",
    "    if np.any(X2):\n",
    "        fig, (ax0, ax1) = plt.subplots(1,2)\n",
    "        if title:\n",
    "            fig.suptitle(title)\n",
    "    else:\n",
    "        ax0 = ax or plt.gca()\n",
    "        if title:\n",
    "            ax0.set_title(title)\n",
    "\n",
    "    # Plot the training points\n",
    "    ax0.scatter(X1[:, 0], X1[:, 1], c=y1, s=30, cmap=cmap,\n",
    "                clim=(y1.min(), y1.max()), zorder=3)\n",
    "    ax0.axis('tight')\n",
    "    ax0.axis('off')\n",
    "    xlim = ax0.get_xlim()\n",
    "    ylim = ax0.get_ylim()\n",
    "\n",
    "    # fit the estimator\n",
    "    model.fit(X1, y1)\n",
    "    xx, yy = np.meshgrid(np.linspace(*xlim, num=200),\n",
    "                            np.linspace(*ylim, num=200))\n",
    "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n",
    "\n",
    "    # Create a color plot with the results\n",
    "    n_classes = len(np.unique(y1))\n",
    "    contours = ax0.contourf(xx, yy, Z, alpha=0.3,\n",
    "                            levels=np.arange(n_classes + 1) - 0.5,\n",
    "                            cmap=cmap, zorder=1)\n",
    "\n",
    "    ax0.set(xlim=xlim, ylim=ylim)\n",
    "\n",
    "\n",
    "    if np.any(X2): \n",
    "\n",
    "\n",
    "        # Plot the training points\n",
    "        ax1.scatter(X2[:, 0], X2[:, 1], c=y2, s=30, cmap=cmap,\n",
    "                    clim=(y.min(), y.max()), zorder=3)\n",
    "        ax1.axis('tight')\n",
    "        ax1.axis('off')\n",
    "        xlim = ax1.get_xlim()\n",
    "        ylim = ax1.get_ylim()\n",
    "\n",
    "        # fit the estimator\n",
    "        model.fit(X2, y2)\n",
    "        xx, yy = np.meshgrid(np.linspace(*xlim, num=200),\n",
    "                                np.linspace(*ylim, num=200))\n",
    "        Z = model.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n",
    "\n",
    "        # Create a color plot with the results\n",
    "        n_classes = len(np.unique(y2))\n",
    "        contours = ax1.contourf(xx, yy, Z, alpha=0.3,\n",
    "                                levels=np.arange(n_classes + 1) - 0.5,\n",
    "                                cmap=cmap, zorder=1)\n",
    "\n",
    "        ax1.set(xlim=xlim, ylim=ylim)\n",
    "\n",
    "def bar_plot(X, y, clfs):\n",
    "    # cross validation\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "    # calculate test/train accuracy for each \n",
    "    scores={}\n",
    "    for name, clf in clfs:\n",
    "        clf.fit(X, y)\n",
    "        scores[name] = ( clf.score(X_train, y_train), clf.score(X_test, y_test) )\n",
    "    # plot in a bar graph\n",
    "    index = np.arange(len(scores.keys()))\n",
    "    bar_width = .35\n",
    "    opacity = .4\n",
    "\n",
    "    #-----------------Enter your Code Here--------------#\n",
    "    train = [a for a,b in scores.values()]\n",
    "    # test should be the other of the pair\n",
    "    test = []\n",
    "    # fill in the bar for train\n",
    "    plt.bar()\n",
    "    plt.bar(index + bar_width, test, bar_width,\n",
    "            alpha=opacity, color = 'b', label = 'Test')\n",
    "    #---------------------------------------------------#\n",
    "\n",
    "    plt.ylim((.85,1.05))\n",
    "    ax = plt.gca()\n",
    "    names = [a for a, b in clfs]\n",
    "    ax.set_xticks(index + bar_width / 2)\n",
    "    ax.set_xticklabels(names)\n",
    "    plt.xlabel(\"Classifier (MEDV > mean)\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.title(\"Boston Housing Classifier Accuracy (MEDV > mean\")\n",
    "    plt.legend()\n",
    "    plt.grid(b=True, which='both')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Trees\n",
    "A decision tree works by taking our data and, by looking for the feature (i.e.\n",
    "rooms in house, distance to work) that most differentiates the data, bifurcates\n",
    "continually until it reaches a single training example. An example will help\n",
    "to illustrate:\n",
    "\n",
    "![House Decision Tree](dt_house.png)\n",
    "\n",
    "The tree will try to group similar training examples together by asking  \n",
    "questions that are the most effective separators. We'll use a decision tree \n",
    "to separate a toy data-set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_blobs(n_samples = 500, centers=4, random_state=0, cluster_std=1.0)\n",
    "plt.scatter(X[:,0], X[:,1], c=y, cmap='rainbow')\n",
    "plt.title(\"Toy Data\")\n",
    "plt.show()\n",
    "\n",
    "title = \"Toy Data: Decision Tree Classifier\"\n",
    "visualize_classifier(DecisionTreeClassifier(), X, y, title = title)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The edges have no question about them, but once the data starts overlapping,\n",
    "the decision tree is more unsure of what to do. We see that play out with the \n",
    "strange small slices in the middle of the data, where the groups come together.\n",
    "It turns out this is a major flaw of decision trees, that it tends to learn \n",
    "the noise within a data-set and is highly sensitive to changes in the data. This\n",
    "is another way of describing overfitting.\n",
    "\n",
    "To further illustrate, we'll take two random subsamples of our data, classify\n",
    "them with decision trees, then compare the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subsamples Decision Tree Classifier\n",
    "\n",
    "ind = np.arange(500)\n",
    "np.random.shuffle(ind)\n",
    "X_samp1, y_samp1 = X[ind[:250], :], y[ind[:250]]\n",
    "np.random.shuffle(ind)\n",
    "X_samp2, y_samp2 = X[ind[:250], :], y[ind[:250]]\n",
    "\n",
    "\n",
    "title = \"Decision Tree on Two Random Subsamples of Toy Data\"\n",
    "clf = DecisionTreeClassifier()\n",
    "visualize_classifier(clf, X_samp1, y_samp1, \n",
    "                     X2 = X_samp2, y2 = y_samp2, title=title)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two things: they agree on the edges, but start to disagree in the middle, where\n",
    "they are unsure. These two samples come from the same data-set, yet we get a \n",
    "fair amount of variation in classification, which illustrates the sensitivity to different\n",
    "data-overfitting-of the Decision Tree algorithm. Additionally, each decision \n",
    "tree uses all the features available to classify the data . This creates a \n",
    "correlation between trees, which we will solve with a random forest. The random\n",
    "forest will randomize the features each decision tree classifies with, which \n",
    "helps remedy tree correlation. \n",
    "\n",
    "\n",
    "## Bagging \n",
    "\n",
    "We can tackle overfitting by taking/bootstrapping samples with \n",
    "replacement from our training data, classifying them with decision trees, then \n",
    "taking the average for a bagged regressor and a majority vote for a bagged \n",
    "classifier. \n",
    "\n",
    "\n",
    "We'll illustrate bagging by coming back to our friend the real estate agent and \n",
    "her Boston Housing data to predict median housing value (MEDV):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston = load_boston()\n",
    "X, y = boston.data, boston.target\n",
    "regr = BaggingRegressor(n_estimators=100, random_state=0)\n",
    "title = (\"True vs. Predicted MEDV by Bagged Decision Tree Regressor\")\n",
    "plot_regr(X, y, regr, title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forests\n",
    "\n",
    "Random forests fix the correlation issue among trees by taking a random subset\n",
    "of features to use for each decision tree to classify with. Let's now compare \n",
    "that to a random forest regressor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regr = RandomForestRegressor(n_estimators = 100, max_depth = 10)\n",
    "title = (\"True vs. Predicted MEDV by Random Forest Regressor\")\n",
    "plot_regr(X, y, regr, title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random forest classifier for MEDV > mean:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(n_estimators = 100, max_depth = 10)\n",
    "y_mean = classify(y, y.mean())\n",
    "title = \"Random Forest Classifier for houses greater or less than mean\"\n",
    "\n",
    "skp.estimators.plot_learning_curve(clf, X, y_mean, title, cv = 5)\n",
    "ax = plt.gca()\n",
    "ax.set_ylabel(\"Accuracy (correct/total)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosting\n",
    "Bagging takes the average of many decision trees. In contrast, boosting trains\n",
    "a tree on equally-weighted training examples, then trains another on a training\n",
    "set with modified weights: it increases the weights of misclassified data. It\n",
    "continues this process until it arrives at a fairly accurate\n",
    "regressor/classifier. \n",
    "\n",
    "Both regularization and boosting train the models to focus on specific areas.In \n",
    "regularization, large parameters get penalized, so the model learns to reduce \n",
    "them. Similarly, boosting trains the model to focus on the misclassified data\n",
    "points, making sure they get the best classification possible.  \n",
    "\n",
    "Let's get a better feel for boosting with a regressor and classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regr = GradientBoostingRegressor()\n",
    "title = (\"True vs. Predicted MEDV by Gradient Boosting Regressor\")\n",
    "plot_regr(X, y, regr, title)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = GradientBoostingClassifier()\n",
    "title = \"Gradient Boosting Classifier for houses greater or less than mean\"\n",
    "skp.estimators.plot_learning_curve(clf, X, y_mean, title, cv = 5)\n",
    "ax = plt.gca()\n",
    "ax.set_ylabel(\"Accuracy (correct/total)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "Let's compare all the new methods we investigated. First, you'll need to recall\n",
    "material from week one on plotting a bar chart and fill in some missing\n",
    "information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test/train accuracies for each model\n",
    "clfs = [('Decision Tree', DecisionTreeClassifier()),\n",
    "        ('Bagged DT' , BaggingClassifier()),\n",
    "        ('Random Forest', RandomForestClassifier()),\n",
    "        ('Gradient Boost', GradientBoostingClassifier())\n",
    "       ]\n",
    "\n",
    "def bar_plot(X, y, clfs):\n",
    "    # cross validation\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "    # calculate test/train accuracy for each \n",
    "    scores={}\n",
    "    for name, clf in clfs:\n",
    "        clf.fit(X, y)\n",
    "        scores[name] = ( clf.score(X_train, y_train), clf.score(X_test, y_test) )\n",
    "    # plot in a bar graph\n",
    "    index = np.arange(len(scores.keys()))\n",
    "    bar_width = .35\n",
    "    opacity = .4\n",
    "\n",
    "    #-----------------Enter your Code Here--------------#\n",
    "    train = [a for a,b in scores.values()]\n",
    "    # grab the test values from scores\n",
    "    test = []\n",
    "    # fill in the bar for train\n",
    "    plt.bar()\n",
    "    plt.bar(index + bar_width, test, bar_width,\n",
    "            alpha=opacity, color = 'b', label = 'Test')\n",
    "    #---------------------------------------------------#\n",
    "\n",
    "    plt.ylim((.85,1.05))\n",
    "    ax = plt.gca()\n",
    "    names = [a for a, b in clfs]\n",
    "    ax.set_xticks(index + bar_width / 2)\n",
    "    ax.set_xticklabels(names)\n",
    "    plt.xlabel(\"Classifier (MEDV > mean)\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.title(\"Boston Housing Classifier Accuracy (MEDV > mean\")\n",
    "    plt.legend()\n",
    "    plt.grid(b=True, which='both')\n",
    "    plt.show()\n",
    "bar_plot(X, y_mean, clfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "To sum up, we saw a bagged model, random forest, and gradient boosting models\n",
    "all applied to Boston Housing data. As you can see, the probabilities are\n",
    "fairly close together, so deciding which to use on a particular dataset isn't \n",
    "clearcut. Machine learning still has a degree of art to it, so it's important\n",
    "to experiment and stay curious when approaching a new analysis. All these\n",
    "models were examples of supervised learning: we gave them labelled data \n",
    "(MEDV > mean or not). Next time, we will explore unsupervised learning, where\n",
    "the training data will not be labelled."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
