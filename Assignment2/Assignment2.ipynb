{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Telling a Story With Data\n",
    "Business often needs to make decisions now about spending money for the future, \n",
    "yet the uncertainty of the future makes this difficult. Being able to make\n",
    "projections about the future can make this task easier. We'll explore examples\n",
    "from a food truck and exam scores to illustrate the power of prediction.\n",
    "\n",
    "## Infrastructure\n",
    "We'll load all the libraries and custom built functions here to avoid clutter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scikitplot.metrics import plot_roc\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "\n",
    "def plot_regr(X, y, regr):\n",
    "    regr.fit(X, y)\n",
    "    plt.plot(X, y, 'o', label = 'Data')\n",
    "    y_pred = regr.predict(X)\n",
    "    label = 'y = %.2f + %.2f*x' % (regr.intercept_, regr.coef_)\n",
    "    plt.plot(X, y_pred, '-b', label = label)\n",
    "    title = \"Truck Profit (y) vs. City Population (x)\"\n",
    "    plt.xlabel(\"City Population (10,000s)\")\n",
    "    plt.ylabel(\"Truck Profit ($10,000s)\")\n",
    "    y_pred = regr.predict(X)\n",
    "    text = \"$R^2$ = %.2f\" % r2_score(y_pred, y)\n",
    "    plt.text(13.75, 21, text)\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def plot_log(X, y, clf):\n",
    "    clf.fit(X, y)\n",
    "\n",
    "    plt.figure(figsize = (8,8))\n",
    "    passed = np.where(y == 1)[0]\n",
    "    failed = np.where(y == 0)[0]\n",
    "    plt.plot(X[passed,0], X[passed,1], '+', label = 'Passed')\n",
    "    plt.plot(X[failed,0], X[failed,1], 'o', label = 'Failed')\n",
    "    plt.xlabel(\"Exam 1 Score\")\n",
    "    plt.ylabel(\"Exam 2 Score\")\n",
    "    plt.title(\"Predicting Pass Rates with Logistic Regression\")\n",
    "    coeff = np.insert(clf.coef_, 0, clf.intercept_)\n",
    "    coeff = coeff / coeff[2]\n",
    "    coeff[:2] = -1 * coeff[:2]\n",
    "    pred = lambda x: coeff[0] + coeff[1]*x\n",
    "    x_min, x_max = X[:,0].min(), X[:,0].max() \n",
    "    t = np.arange(x_min, x_max)\n",
    "    plt.plot(t, pred(t), '-b', label = 'Decision Boundary')\n",
    "    text = 'y = %.2f + %.2f*x' % (coeff[0], coeff[1])\n",
    "    plt.text(60, 30, text)\n",
    "    y_pred = clf.predict(X)\n",
    "    text2 = \"$R^2$ = %.2f\" % r2_score(y_pred, y)\n",
    "    plt.text(60, 25, text2)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def plot_sigmoid():\n",
    "    s = lambda x: 1 / (1 + np.exp(-x))\n",
    "    t = np.arange(-10, 10, .5)\n",
    "    plt.plot(t, s(t), '-b')\n",
    "    a = np.arange(-10, 0, .5)\n",
    "    plt.plot(a, [0.5]*len(a), '--r')\n",
    "    b = np.arange(0, 0.5, .01)\n",
    "    plt.plot([0]*len(b), b, '--r')\n",
    "    plt.annotate('A common threshold: 50%', xy=(0, 0.5), xytext=(1, .65),\n",
    "                 arrowprops=dict(facecolor='black', shrink=0.05))\n",
    "    plt.title(\"The Sigmoid Function\")\n",
    "    plt.xlabel(\"$X$\")\n",
    "    plt.ylabel(\"$P(X)$\")\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "def plot_gd(X, y, theta, Js):\n",
    "    fig, (ax0, ax1) = plt.subplots(1,2, figsize = (10, 10))\n",
    "    ax0.plot(X, y, 'o', label = 'Data')\n",
    "    label = 'y = %.2f + %.2f*x' % (theta[0], theta[1])\n",
    "    # mod X to make math work\n",
    "    X_mod = np.insert(X, 0, 1, axis = 1)\n",
    "    y_pred = X_mod.dot(theta)\n",
    "    ax0.plot(X, y_pred, '-b', label = label)\n",
    "    ax0.set_xlabel(\"City Population (10,000s)\")\n",
    "\n",
    "    text = \"$R^2$ = %.2f\" % r2_score(y_pred, y)\n",
    "    ax0.text(6, 22, text)\n",
    "    ax0.set_ylabel(\"Truck Profit ($10,000s)\")\n",
    "    ax0.set_title(\"Truck Profit (y) vs. City Population (x)\")\n",
    "    ax0.legend()\n",
    "\n",
    "    t = np.arange(1, len(Js)+1)\n",
    "    ax1.plot(t, Js, '.') \n",
    "    ax1.set_xlabel(\"Iteration\")\n",
    "    ax1.set_ylabel(\"Cost (MSE)\")\n",
    "    ax1.set_title(\"Gradient Descent Cost Minimization\")\n",
    "    plt.show()\n",
    "\n",
    "def gradient_descent(X, y, theta, alpha, iterations):\n",
    "    # insert columns of ones to make math work\n",
    "    X = np.insert(X, 0, 1, axis = 1)\n",
    "    n_rows, n_cols = X.shape\n",
    "    J_hist = np.zeros(iterations)\n",
    "\n",
    "    for _ in range(iterations):\n",
    "\n",
    "        J = 0\n",
    "        temps = np.zeros(n_cols)\n",
    "        grads = np.zeros(n_cols)\n",
    "\n",
    "        # our prediction\n",
    "        h = lambda a: a.dot(theta)\n",
    "\n",
    "        # cost function is mean-squared-error\n",
    "        for i in range(n_rows):\n",
    "            J += ( h(X[i, :]) - y[i] )**2 / (2*n_rows)\n",
    "        J_hist[_] = J\n",
    "\n",
    "        # compute gradients and place in temp. variable\n",
    "        for j in range(n_cols):\n",
    "            for i in range(n_rows):\n",
    "                grads[j] += ( h(X[i, :]) - y[i] ) * X[i,j] / n_rows\n",
    "\n",
    "            temps[j] = theta[j]- alpha*grads[j]\n",
    "\n",
    "        # simultaneous update\n",
    "        for j in range(n_cols):\n",
    "            theta[j] = temps[j]\n",
    "\n",
    "    return theta, J_hist\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression with sklearn\n",
    "It's best to first get a feel for what's going on by seeing the output of this\n",
    "procedure first so that we don't get bogged down in the details. Sklearn is a\n",
    "pre-built library we can use so that we don't have to make our own\n",
    "implementations of these tools. We'll examine the case of a food truck owner who\n",
    "is interested in expanding to a new city, but wants to gain more confidence on\n",
    "his chances of probability before making the leap:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "address = 'https://raw.githubusercontent.com/' +\\\n",
    "          'geofflangenderfer/data/master/Assignment2/ex1data1.txt'\n",
    "data = pd.read_csv(address, header = None).values\n",
    "\n",
    "X, y = data[:,:-1], data[:,-1]\n",
    "regr = linear_model.LinearRegression()\n",
    "plot_regr(X, y, regr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the truck owner has the population for the considered town, he can make a\n",
    "prediction on how profitable his truck will be based on historical data. It\n",
    "won't be 100% accurate, but it gives him a tool that reduces uncertainty. \n",
    "\n",
    "\n",
    "\n",
    "## Linear Regression with custom Gradient Descent\n",
    "Linear regression works under the hood by minimizing the residuals between the\n",
    "prediction and true values. This cost function  is minimized with an algorithm\n",
    "called gradient descent. \n",
    "\n",
    "Pretend you're sitting on the top of the hill,blind-folded, and want to get down\n",
    "from the hill. Which direction should you go? You feel around and one direction\n",
    "leads higher up and another descends lower. You take the direction header\n",
    "lower, while continuing to feel for the direction of steepest descent, until\n",
    "you reach the bottom of the hill. This is what happens in gradient descent,\n",
    "until we reach the minimum of the cost function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_theta = np.zeros(2)\n",
    "learning_rate = .01\n",
    "iterations = 1000\n",
    "\n",
    "theta, J_hist = gradient_descent(X, y, initial_theta, \n",
    "                                learning_rate, iterations)\n",
    "\n",
    "plot_gd(X, y, theta, J_hist)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the answers from our custom Gradient Descent algorithm and\n",
    "sklearn are fairly equal. Though, using the package does allow us to get to our\n",
    "prediction quicker. \n",
    "\n",
    "## Interpreting Linear Regression\n",
    "Our graphs produced an equation for our prediction and a $R^2$ value. the\n",
    "coefficients of the equation describe the sensitivity of profit to changes in\n",
    "population:\n",
    "\n",
    "- the intercept says that if you go to a town without any residents, you will\n",
    "  lose $32,400.\n",
    "\n",
    "- the coefficient of x says that for an increase in city population  by 10,000, \n",
    "  the truck will make an additional $11,300.\n",
    "\n",
    "The $R^2$ value describes how well our prediction models fit the data used to \n",
    "train it. $R^2$:\n",
    "\n",
    "- is in [0,1]\n",
    "- the closer to one, the more the model explains the variance in the data. \n",
    "\n",
    "\n",
    "## Logistic Regression with sklearn\n",
    "Suppose you have a person's weight and age, and you'd like to\n",
    "predict his probability of heart attack within one year. You could use a tool\n",
    "like logistic regression to classify the event as likely or not. \n",
    "\n",
    "We'll explore a dataset of exam scores labelled as either passing or failing to\n",
    "explore this further:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "address = 'https://raw.githubusercontent.com/geofflangenderfer/' +\\\n",
    "          'data/master/Assignment2/ex2data1.txt'\n",
    "data = pd.read_csv(address, header = None).values\n",
    "X = data[:,:-1]; y = data[:,-1]\n",
    "clf = linear_model.LogisticRegression(solver='lbfgs')\n",
    "plot_log(X, y, clf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "Say a student scored 50 on the first exam because she was sick. She thinks she\n",
    "can do better on the next one with a 90. What are the chances she passes the\n",
    "class?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_array = np.array([[50, 90]])\n",
    "prob_pass = clf.predict(data_array)\n",
    "print(\"The probability of passing with %s: %.2f\" % (data_array, prob_pass))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the chances a student passes with a 60 on exam 1 and 64 on exam 2?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------Enter Your Code Here------------#\n",
    "data_array = \n",
    "prob_pass = \n",
    "print(\"The probability of passing with %s: %.2f\" % (data_array, prob_pass))\n",
    "#-------------------------------------------#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpreting Logistic Regression\n",
    "\n",
    "### The Sigmoid Function\n",
    "Logistic regression works similarly to linear by finding weights through\n",
    "gradient descent that minimize its cost function (something different than the\n",
    "residuals). It takes the dot product of exam scores and solved-coefficients to\n",
    "come up with a real-value. Next, it turns that value into a probability by \n",
    "passing it through the sigmoid function. Finally, each training example of exam\n",
    "scores is classified as passing or not based on a chosen probability threshold,\n",
    "usually 50%. \n",
    "\n",
    "The sigmoid function is very useful because it maps all real-values to (0, 1).\n",
    "Let's take a look to see that visually:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_sigmoid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, no matter how large X, P(X) goes no higher than one. Likewise,\n",
    "no matter how small X, P(x) goes no lower than 0. \n",
    "\n",
    "### The Coefficients\n",
    "The process for interpreting the coefficients in logistic regression is similar \n",
    "to linear. The difference is we talk in terms of percentages instead of\n",
    "real-values.\n",
    "\n",
    "- The intercept can be interpreted as the as the minimum exam 2 score required\n",
    "  to pass. If you score 0 on exam 1, our model says you need to score ~125%\n",
    "  on exam 2-a tall task.\n",
    "- the coefficient of x is interpreted as the change in exam 2 score required \n",
    "  to pass for a 1% increase in exam 1 score. If you score 1% higher on exam 1,\n",
    "  you will need 1.02% less on exam 2 in order to pass.\n",
    "\n",
    "\n",
    "### ROC curves\n",
    "These give us a way to compare the rate of correctly passing exam scores\n",
    "(predicted) to the rate of falsely passing exam scores (predicted). If you\n",
    "change the classification threshold (usually 50%), these values will vary, which is what\n",
    "produces the curve. Let's take a look at the ROC for the exam scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.fit(X, y).predict_proba(X)\n",
    "        \n",
    "# FPR(T) and TPR(T) are functions of the acceptance threshold T\n",
    "# the roc plots the (fpr, tpr) pairs that result from varying T\n",
    "plot_roc(y, y_pred)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you look at the point (0,0.6) for class 1, this is telling you that for the\n",
    "chosen pass threshold, you can expect a false-positive rate of 0% and a\n",
    "true-positive rate of 60%. A perfect score would be 100% and this would happen\n",
    "if the curve was in the far left corner of the graph. This also corresponds to \n",
    "an area under the curve of 1.\n",
    "\n",
    "Google is famous for not wanting any false positives in their hiring \n",
    "decisions, so they might choose a similar threshold with a 0 FPR.\n",
    "\n",
    "## Conclusion\n",
    "We saw how a food truck owner can get an idea for how profitable a new truck in\n",
    "a new city can do with linear regression. And, we saw how a student could\n",
    "predict what score they would need to be fairly confident they would pass.\n",
    "These tools are effective at giving an idea about the future, but business data\n",
    "is often more complicated, with more variables. We'll learn more about the\n",
    "multivariate cases for these tools next time."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
